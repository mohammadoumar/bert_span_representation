{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b005d6",
   "metadata": {},
   "source": [
    "# Implementation of the Kuribayashi BERT minus model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e794073",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e0f3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.4)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: IProgress in /opt/conda/lib/python3.8/site-packages (0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from IProgress) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (1.15.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch-lr-finder in /opt/conda/lib/python3.8/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from torch-lr-finder) (1.10.0a0+0aef44c)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from torch-lr-finder) (3.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch-lr-finder) (1.21.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from torch-lr-finder) (21.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torch-lr-finder) (4.62.3)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=0.4.1->torch-lr-finder) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->torch-lr-finder) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->torch-lr-finder) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->torch-lr-finder) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->torch-lr-finder) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->torch-lr-finder) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->torch-lr-finder) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --upgrade\n",
    "!pip install ipywidgets\n",
    "!pip install IProgress\n",
    "!pip install datasets\n",
    "!pip install torch-lr-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2b768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "from transformers import BatchEncoding, default_data_collator, DataCollatorWithPadding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f140bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28885918",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fa4276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc21695",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb3e5bc",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "0c699094-439f-4dda-85a9-815e7948540c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_FOLDER = '/notebooks/Data/bert_sequence_classification'\n",
    "DATA_FILE = '/notebooks/KURI-BERT/data/pe_dataset_for_bert_minus.pt'\n",
    "RESULTS_FOLDER = '/notebooks/KURI-BERT/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfac076",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "35aada91-232a-421e-a28f-94b359c6d65d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9628fd11",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "dc52e71e-65fa-4946-acdf-e4fffe9d0f79",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e213e",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "013a6d64-65e7-4189-a468-40ecfd8a6736",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60790fe1",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "626737f9-ff56-4992-b72b-60750375e455",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94eb648",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "4fbfcc7b-55fb-456e-ab02-2ae975803046",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['paragraph', 'paragraph_components_list', 'paragraph_labels_list', 'paragraph_markers_list', 'split', 'essay_nr', 'paragraph_labels', 'paragraph_ac_spans', 'paragraph_am_spans'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['paragraph', 'paragraph_components_list', 'paragraph_labels_list', 'paragraph_markers_list', 'split', 'essay_nr', 'paragraph_labels', 'paragraph_ac_spans', 'paragraph_am_spans'],\n",
       "        num_rows: 358\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['paragraph', 'paragraph_components_list', 'paragraph_labels_list', 'paragraph_markers_list', 'split', 'essay_nr', 'paragraph_labels', 'paragraph_ac_spans', 'paragraph_am_spans'],\n",
       "        num_rows: 273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a88d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 23], [25, 46], [48, 68], [72, 82]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['paragraph_ac_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5d3b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3], [-1, -1], [-1, -1], [70, 71]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['paragraph_am_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c0bd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 11], [18, 26], [28, 36]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][410]['paragraph_ac_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bf28888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, -1], [15, 15], [-1, -1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][410]['paragraph_am_spans']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af3295",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e16bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 0\n",
    "\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    \n",
    "    for col_name in ['paragraph_am_spans', 'paragraph_ac_spans']:\n",
    "        \n",
    "        for x in dataset[split][col_name]:\n",
    "        \n",
    "            if len(x) > MAX_LENGTH:\n",
    "                \n",
    "                MAX_LENGTH = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a36fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(batch, padding_target):    \n",
    "    \n",
    "    if padding_target == 'am_spans':\n",
    "        \n",
    "        col_name = 'paragraph_am_spans'\n",
    "        padding_val = [[-1,-1]]\n",
    "        max_length = MAX_LENGTH\n",
    "        \n",
    "    elif padding_target == 'ac_spans':\n",
    "        \n",
    "        col_name = 'paragraph_ac_spans'\n",
    "        padding_val = [[-1,-1]]\n",
    "        max_length = MAX_LENGTH\n",
    "        \n",
    "    elif padding_target == 'label':\n",
    "    \n",
    "        col_name = 'paragraph_labels'\n",
    "        padding_val = [-100] # -1 previously       \n",
    "        max_length = MAX_LENGTH # max([len(l) for l in batch[col_name]]) # cause some batch had 4 x 10\n",
    "\n",
    "    padded_spans = []\n",
    "\n",
    "    for idx, span in enumerate(batch[col_name]):\n",
    "\n",
    "        padded_span = batch[col_name][idx] + (max_length - len(span)) * padding_val\n",
    "        padded_spans.append(padded_span)\n",
    "\n",
    "    return padded_spans         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf442400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_combined_spans(am_spans_ll, ac_spans_ll):\n",
    "    \n",
    "#     spans_ll = []\n",
    "    \n",
    "#     for am_spans, ac_spans in zip(am_spans_ll, ac_spans_ll):\n",
    "        \n",
    "#         spans = []\n",
    "        \n",
    "#         for am_span, ac_span in zip(am_spans, ac_spans):\n",
    "\n",
    "#             span = [am_span, ac_span]\n",
    "#             spans.extend(span)\n",
    "            \n",
    "#         spans_ll.append(spans)\n",
    "\n",
    "#     return spans_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbe7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_adu_spans(am_spans_ll, ac_spans_ll):\n",
    "    \n",
    "    joint_adu_spans_ll = []\n",
    "    \n",
    "    for am_spans, ac_spans in zip(am_spans_ll, ac_spans_ll):\n",
    "        \n",
    "        spans = []\n",
    "        \n",
    "        for am_span, ac_span in zip(am_spans, ac_spans):\n",
    "            \n",
    "            if am_span == [-1 , -1]:\n",
    "                \n",
    "                span = ac_span\n",
    "                spans.extend(span)\n",
    "            \n",
    "            else:\n",
    "                # print('am span [0]', am_span[0])\n",
    "                span = [am_span[0], ac_span[1]]\n",
    "                # print(span)\n",
    "                spans.extend(span)\n",
    "                \n",
    "        joint_adu_spans_ll.append(spans)\n",
    "        \n",
    "        # joint_adu_spans_ll = [list((joint_adu_spans_ll[i], joint_adu_spans_ll[i+1])) for i in range(0, len(joint_adu_spans_ll) - 1, 2)]\n",
    "        \n",
    "\n",
    "        \n",
    "    return joint_adu_spans_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb3e13",
   "metadata": {},
   "source": [
    "### tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb6072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 200 for use in the max_length in the tokenizer so that the things are of equal dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db293287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    \n",
    "    tokenized_text = tokenizer(batch['paragraph'], truncation=True, padding=True, max_length=512)\n",
    "    tokenized_text['label'] = get_padding(batch, 'label')\n",
    "    tokenized_text['am_spans'] = get_padding(batch, 'am_spans')\n",
    "    tokenized_text['ac_spans'] = get_padding(batch, 'ac_spans')\n",
    "    \n",
    "    tokenized_text['spans'] = get_joint_adu_spans(tokenized_text['am_spans'], tokenized_text['ac_spans'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27efb06d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f0ccd811dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01802968978881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d120894c3f4acd877210592a1f79b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01704573631286621,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae256b45a94645099d6f16aa1d743ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017505168914794922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dfc10d950d49c987f87713317fd404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec1c9a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ac_spans', 'am_spans', 'attention_mask', 'essay_nr', 'input_ids', 'label', 'paragraph', 'paragraph_ac_spans', 'paragraph_am_spans', 'paragraph_components_list', 'paragraph_labels', 'paragraph_labels_list', 'paragraph_markers_list', 'spans', 'split', 'token_type_ids'],\n",
       "        num_rows: 1088\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ac_spans', 'am_spans', 'attention_mask', 'essay_nr', 'input_ids', 'label', 'paragraph', 'paragraph_ac_spans', 'paragraph_am_spans', 'paragraph_components_list', 'paragraph_labels', 'paragraph_labels_list', 'paragraph_markers_list', 'spans', 'split', 'token_type_ids'],\n",
       "        num_rows: 358\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ac_spans', 'am_spans', 'attention_mask', 'essay_nr', 'input_ids', 'label', 'paragraph', 'paragraph_ac_spans', 'paragraph_am_spans', 'paragraph_components_list', 'paragraph_labels', 'paragraph_labels_list', 'paragraph_markers_list', 'spans', 'split', 'token_type_ids'],\n",
       "        num_rows: 273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f257b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [-1, -1], [-1, -1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['paragraph_am_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233eda0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 21], [27, 43], [45, 80]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['paragraph_ac_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47ff62d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 21,\n",
       " 27,\n",
       " 43,\n",
       " 45,\n",
       " 80,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52a960e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### some work"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b264bae",
   "metadata": {},
   "source": [
    "x = dataset['train'][0]['spans']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4ec5389",
   "metadata": {},
   "source": [
    "y = [list((x[i], x[i+1])) for i in range(0, len(x) - 1, 2)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "506a99e4",
   "metadata": {},
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcbd4c36",
   "metadata": {},
   "source": [
    "# try a second fx here"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4c6a5d7",
   "metadata": {},
   "source": [
    "def join_spans(batch):\n",
    "    \n",
    "    adu_spans = []\n",
    "        \n",
    "    for i in range(0, len(batch['spans']) - 1, 2):\n",
    "        tmp_span = list((joint_adu_spans_ll[i], joint_adu_spans_ll[i+1]))\n",
    "        adu_spans.append(tmp_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72d4d523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ac_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'am_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'label': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_ac_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_am_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_components_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_labels_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_markers_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'spans': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be17e4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ac_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'am_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'essay_nr': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'label': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph': Value(dtype='string', id=None),\n",
       " 'paragraph_ac_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_am_spans': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_components_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'paragraph_labels_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'paragraph_markers_list': Sequence(feature=Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'spans': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60590c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'].features['spans'] = datasets.Array2D(shape=(24, 1), dtype=\"int32\")\n",
    "dataset['train'].features['spans'] = datasets.Array2D(shape=(24, 1), dtype=\"int32\")\n",
    "dataset['validation'].features['spans'] = datasets.Array2D(shape=(24, 1), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb1c0fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018591880798339844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f0fc1b2fb94ce59dc5e7549cdd1654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017390966415405273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873fd63ac871477b87c6bf194c3e583a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017157316207885742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cd148948ed4ad1ac7e52f63b6cc633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda batch: batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "338ec067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'spans', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0b65177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 21, 27, 43, 45, 80, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d7c8313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['spans'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05ecdf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok now spans are a list of 24 indices from which we will later construct 12 ADU spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf70a12",
   "metadata": {},
   "source": [
    "## span representation function old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c7bb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_span_representations(outputs, spans):\n",
    "\n",
    "#     batch_size = spans.shape[0]\n",
    "#     nr_span_indices = spans.shape[1]\n",
    "    \n",
    "#     # print('nr span indices: ', nr_span_indices)\n",
    "    \n",
    "#     idx_l_ams = range(0, nr_span_indices, 2) # [0,2,4,6 etc]\n",
    "#     idx_l_acs = range(1, nr_span_indices, 2) # [1,3,5,7 etc]\n",
    "    \n",
    "#     am_spans = spans[:, idx_l_ams, :] + 1 # adds 1 to all span indices (both am and ac) to offset for the CLS token in the input_ids.\n",
    "#     ac_spans = spans[:, idx_l_acs, :] + 1\n",
    "    \n",
    "#     am_spans = am_spans.flatten(start_dim=1)\n",
    "#     ac_spans = ac_spans.flatten(start_dim=1)\n",
    "    \n",
    "#     #print(\"am spans:\", am_spans.shape)\n",
    "#     #print(\"ac spans:\", ac_spans.shape)\n",
    "    \n",
    "#     outputs_am = outputs[:,am_spans,:]\n",
    "#     outputs_am = torch.cat([outputs_am[i,i,:,:] for i in range(batch_size)], dim=0)\n",
    "#     outputs_am = outputs_am.reshape(batch_size, nr_span_indices, -1)\n",
    "    \n",
    "#     # print(\"outputs am:\", outputs_am.shape)\n",
    "    \n",
    "#     ### Now that we have outputs_am i.e. outputs at am_span indices, now create the four Kuri forumlas for AMs\n",
    "    \n",
    "#     # ============== FIRST TERM ===================\n",
    "    \n",
    "#     outputs_am_first_term = torch.cat([outputs_am[:,i+1,:] - outputs_am[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1) \n",
    "#     outputs_am_first_term = outputs_am_first_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== SECOND TERM ==================\n",
    "    \n",
    "#     outputs_am_second_term = torch.cat([outputs_am[:,i,:] - outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run # changed from +1 to +2 to ensure +2 is not a problem for AMs\n",
    "#     outputs_am_second_term = outputs_am_second_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    \n",
    "#     # ============== THIRD TERM ==================\n",
    "    \n",
    "#     outputs_am_third_term = torch.cat([outputs_am[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_am_third_term = outputs_am_third_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== FOURTH TERM ==================\n",
    "    \n",
    "#     outputs_am_fourth_term = torch.cat([outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run # changed from +1 to +2 to ensure +2 is not a problem for AMs\n",
    "#     outputs_am_fourth_term = outputs_am_fourth_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== NOW CONCATENATE THEM =========\n",
    "    \n",
    "    \n",
    "#     am_minus_representations = torch.cat([outputs_am_first_term, outputs_am_second_term, outputs_am_third_term, outputs_am_fourth_term], dim=-1)   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ### am minus span representation according to kuribayashi paper is now here.\n",
    "    \n",
    "#     ### ========================================= OLD AM CALCULATIONS ==========================\n",
    "    \n",
    "# #     outputs_am_r = torch.cat([outputs_am[:,i,:] - outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "# #     outputs_am_r = outputs_am_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "# #     outputs_am_l = torch.cat([outputs_am[:,i+1,:] - outputs_am[:,i,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "# #     outputs_am_l = outputs_am_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "# #     am_minus_representations = torch.cat([outputs_am_r, outputs_am_l], dim=-1)\n",
    "    \n",
    "#     #print(\"am_minus_representations:\", am_minus_representations.shape)\n",
    "    \n",
    "#     ### ====================================== FIN OLD AM CALCULATIONS =========================\n",
    "    \n",
    "#     outputs_ac = outputs[:,ac_spans,:]\n",
    "#     outputs_ac = torch.cat([outputs_ac[i,i,:,:] for i in range(batch_size)], dim=0)\n",
    "#     outputs_ac = outputs_ac.reshape(batch_size, nr_span_indices, -1)\n",
    "    \n",
    "#     #print(\"outputs ac:\", outputs_ac.shape)\n",
    "    \n",
    "#     ### Now that we have outputs_ac i.e. outputs at ac_span indices, now create the four Kuri forumlas for ACs\n",
    "    \n",
    "    \n",
    "#     # ============== FIRST TERM ===================\n",
    "    \n",
    "#     outputs_ac_first_term = torch.cat([outputs_ac[:,i+1,:] - outputs_ac[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_ac_first_term = outputs_ac_first_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== SECOND TERM ==================\n",
    "    \n",
    "#     outputs_ac_second_term = torch.cat([outputs_ac[:,i,:] - outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run\n",
    "#     outputs_ac_second_term = outputs_ac_second_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    \n",
    "#     # ============== THIRD TERM ==================\n",
    "    \n",
    "#     outputs_ac_third_term = torch.cat([outputs_ac[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_ac_third_term = outputs_ac_third_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== FOURTH TERM ==================\n",
    "    \n",
    "#     outputs_ac_fourth_term = torch.cat([outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run\n",
    "#     outputs_ac_fourth_term = outputs_ac_fourth_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     # ============== NOW CONCATENATE THEM =========\n",
    "    \n",
    "    \n",
    "#     ac_minus_representations = torch.cat([outputs_ac_first_term, outputs_ac_second_term, outputs_ac_third_term, outputs_ac_fourth_term], dim=-1)   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ### ac minus span representation according to kuribayashi paper is now here.\n",
    "    \n",
    "#     ### ========================================= OLD AC CALCULATIONS ==========================\n",
    "    \n",
    "# #     outputs_ac_r = torch.cat([outputs_ac[:,i,:] - outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "# #     outputs_ac_r = outputs_ac_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "# #     outputs_ac_l = torch.cat([outputs_ac[:,i+1,:] - outputs_ac[:,i,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "# #     outputs_ac_l = outputs_ac_l.reshape(batch_size, -1, 768)\n",
    "    \n",
    "# #     ac_minus_representations = torch.cat([outputs_ac_r, outputs_ac_l], dim=-1)\n",
    "\n",
    "#     ### ====================================== FIN OLD AC CALCULATIONS =========================\n",
    "    \n",
    "#     #print(\"ac_minus_representations:\", ac_minus_representations.shape)\n",
    "    \n",
    "#     return am_minus_representations, ac_minus_representations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "348b54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensy = torch.rand(48, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e0a7441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 24])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4947453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensy_l = tensy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff0892c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.17168915271759033,\n",
       "  0.302817165851593,\n",
       "  0.23487138748168945,\n",
       "  0.11001861095428467,\n",
       "  0.19454807043075562,\n",
       "  0.9708924293518066,\n",
       "  0.8996075987815857,\n",
       "  0.8302733898162842,\n",
       "  0.5763616561889648,\n",
       "  0.6052727103233337,\n",
       "  0.20066696405410767,\n",
       "  0.24728244543075562,\n",
       "  0.7759718894958496,\n",
       "  0.706483006477356,\n",
       "  0.19881552457809448,\n",
       "  0.3861386179924011,\n",
       "  0.7213626503944397,\n",
       "  0.6232663989067078,\n",
       "  0.36151885986328125,\n",
       "  0.5948505401611328,\n",
       "  0.7508516311645508,\n",
       "  0.2725784182548523,\n",
       "  0.026015818119049072,\n",
       "  0.6215766668319702],\n",
       " [0.02072244882583618,\n",
       "  0.4277816414833069,\n",
       "  0.42437058687210083,\n",
       "  0.9133836030960083,\n",
       "  0.894695520401001,\n",
       "  0.40514326095581055,\n",
       "  0.12138873338699341,\n",
       "  0.03523838520050049,\n",
       "  0.637154757976532,\n",
       "  0.1049647331237793,\n",
       "  0.17650604248046875,\n",
       "  0.5680537223815918,\n",
       "  0.5146389603614807,\n",
       "  0.7100184559822083,\n",
       "  0.6107532382011414,\n",
       "  0.5055559873580933,\n",
       "  0.3960137963294983,\n",
       "  0.03858530521392822,\n",
       "  0.3522071838378906,\n",
       "  0.06991809606552124,\n",
       "  0.7755093574523926,\n",
       "  0.863541841506958,\n",
       "  0.23714834451675415,\n",
       "  0.8501376509666443],\n",
       " [0.12422841787338257,\n",
       "  0.7724658846855164,\n",
       "  0.606565535068512,\n",
       "  0.32325422763824463,\n",
       "  0.3566409945487976,\n",
       "  0.5438502430915833,\n",
       "  0.7787052989006042,\n",
       "  0.2408994436264038,\n",
       "  0.6985463500022888,\n",
       "  0.8152540922164917,\n",
       "  0.6523404121398926,\n",
       "  0.3795153498649597,\n",
       "  0.9208999276161194,\n",
       "  0.3006328344345093,\n",
       "  0.853693425655365,\n",
       "  0.7555317878723145,\n",
       "  0.4046822786331177,\n",
       "  0.975095272064209,\n",
       "  0.934761106967926,\n",
       "  0.4659804701805115,\n",
       "  0.5050164461135864,\n",
       "  0.6474541425704956,\n",
       "  0.2170504331588745,\n",
       "  0.166692852973938],\n",
       " [0.24472099542617798,\n",
       "  0.4014214277267456,\n",
       "  0.13364601135253906,\n",
       "  0.47675299644470215,\n",
       "  0.5576980710029602,\n",
       "  0.35181862115859985,\n",
       "  0.9024598002433777,\n",
       "  0.0974850058555603,\n",
       "  0.34284019470214844,\n",
       "  0.6522940993309021,\n",
       "  0.2156085968017578,\n",
       "  0.16868728399276733,\n",
       "  0.7325751185417175,\n",
       "  0.35382795333862305,\n",
       "  0.38057106733322144,\n",
       "  0.4929543733596802,\n",
       "  0.013283312320709229,\n",
       "  0.10836178064346313,\n",
       "  0.6712048649787903,\n",
       "  0.10221725702285767,\n",
       "  0.8894752264022827,\n",
       "  0.767790675163269,\n",
       "  0.6004040837287903,\n",
       "  0.04429209232330322],\n",
       " [0.7637056112289429,\n",
       "  0.7655920386314392,\n",
       "  0.9840447306632996,\n",
       "  0.7797181010246277,\n",
       "  0.30164825916290283,\n",
       "  0.5220595598220825,\n",
       "  0.6425966024398804,\n",
       "  0.1576417088508606,\n",
       "  0.9643210172653198,\n",
       "  0.455949604511261,\n",
       "  0.23938852548599243,\n",
       "  0.8483310341835022,\n",
       "  0.497348427772522,\n",
       "  0.32600241899490356,\n",
       "  0.9982336163520813,\n",
       "  0.9611243605613708,\n",
       "  0.636556088924408,\n",
       "  0.7490946054458618,\n",
       "  0.2319687008857727,\n",
       "  0.059793829917907715,\n",
       "  0.7296817898750305,\n",
       "  0.38178062438964844,\n",
       "  0.025968611240386963,\n",
       "  0.8931936025619507],\n",
       " [0.40517616271972656,\n",
       "  0.1458902359008789,\n",
       "  0.023820579051971436,\n",
       "  0.6724759936332703,\n",
       "  0.5676811337471008,\n",
       "  0.5715808868408203,\n",
       "  0.3215993046760559,\n",
       "  0.7823853492736816,\n",
       "  0.21731233596801758,\n",
       "  0.2605699896812439,\n",
       "  0.3096277117729187,\n",
       "  0.33651208877563477,\n",
       "  0.8131352066993713,\n",
       "  0.09882962703704834,\n",
       "  0.09538155794143677,\n",
       "  0.7711284756660461,\n",
       "  0.4023793935775757,\n",
       "  0.11439526081085205,\n",
       "  0.2173967957496643,\n",
       "  0.5438826680183411,\n",
       "  0.308529257774353,\n",
       "  0.1362897753715515,\n",
       "  0.0737835168838501,\n",
       "  0.3400459289550781],\n",
       " [0.6102308630943298,\n",
       "  0.6759757995605469,\n",
       "  0.8965817093849182,\n",
       "  0.8177597522735596,\n",
       "  0.4744412302970886,\n",
       "  0.5801161527633667,\n",
       "  0.22408944368362427,\n",
       "  0.42200952768325806,\n",
       "  0.7262744307518005,\n",
       "  0.02270972728729248,\n",
       "  0.2617708444595337,\n",
       "  0.992360532283783,\n",
       "  0.8091744184494019,\n",
       "  0.7018395662307739,\n",
       "  0.8307307958602905,\n",
       "  0.9890048503875732,\n",
       "  0.22597986459732056,\n",
       "  0.4836966395378113,\n",
       "  0.16138458251953125,\n",
       "  0.5748518705368042,\n",
       "  0.33556467294692993,\n",
       "  0.2654010057449341,\n",
       "  0.4849807024002075,\n",
       "  0.41523969173431396],\n",
       " [0.350439190864563,\n",
       "  0.20154917240142822,\n",
       "  0.30837082862854004,\n",
       "  0.9825896620750427,\n",
       "  0.7057996392250061,\n",
       "  0.36860567331314087,\n",
       "  0.1850232481956482,\n",
       "  0.1129578948020935,\n",
       "  0.5790365934371948,\n",
       "  0.597599983215332,\n",
       "  0.7316299080848694,\n",
       "  0.19119524955749512,\n",
       "  0.7100183963775635,\n",
       "  0.854689359664917,\n",
       "  0.019701838493347168,\n",
       "  0.3797488808631897,\n",
       "  0.8321186900138855,\n",
       "  0.9242724776268005,\n",
       "  0.6883131265640259,\n",
       "  0.21801412105560303,\n",
       "  0.4232826232910156,\n",
       "  0.9737843871116638,\n",
       "  0.4597240090370178,\n",
       "  0.7678506970405579],\n",
       " [0.20535939931869507,\n",
       "  0.11009126901626587,\n",
       "  0.07486522197723389,\n",
       "  0.09403306245803833,\n",
       "  0.27560514211654663,\n",
       "  0.9378510117530823,\n",
       "  0.7461086511611938,\n",
       "  0.04273957014083862,\n",
       "  0.921593427658081,\n",
       "  0.343855619430542,\n",
       "  0.26891666650772095,\n",
       "  0.5061367154121399,\n",
       "  0.35598331689834595,\n",
       "  0.29854822158813477,\n",
       "  0.017540931701660156,\n",
       "  0.5519097447395325,\n",
       "  0.5086615085601807,\n",
       "  0.7562202215194702,\n",
       "  0.4178076386451721,\n",
       "  0.22784340381622314,\n",
       "  0.6288447976112366,\n",
       "  0.9317689538002014,\n",
       "  0.9463105201721191,\n",
       "  0.8811867237091064],\n",
       " [0.2663917541503906,\n",
       "  0.6182193756103516,\n",
       "  0.8131062388420105,\n",
       "  0.596411406993866,\n",
       "  0.8423597812652588,\n",
       "  0.021668434143066406,\n",
       "  0.7566080093383789,\n",
       "  0.33213353157043457,\n",
       "  0.5247715711593628,\n",
       "  0.9161971807479858,\n",
       "  0.5173883438110352,\n",
       "  0.45366519689559937,\n",
       "  0.1814236044883728,\n",
       "  0.22559189796447754,\n",
       "  0.5095747709274292,\n",
       "  0.549243152141571,\n",
       "  0.11840486526489258,\n",
       "  0.8841248154640198,\n",
       "  0.07628554105758667,\n",
       "  0.15482968091964722,\n",
       "  0.816685140132904,\n",
       "  0.6827566623687744,\n",
       "  0.6540245413780212,\n",
       "  0.1532222032546997],\n",
       " [0.5561574101448059,\n",
       "  0.7712180614471436,\n",
       "  0.3210383653640747,\n",
       "  0.13564151525497437,\n",
       "  0.2535169720649719,\n",
       "  0.05336874723434448,\n",
       "  0.43679171800613403,\n",
       "  0.8281631469726562,\n",
       "  0.957253098487854,\n",
       "  0.5046791434288025,\n",
       "  0.10412168502807617,\n",
       "  0.19023478031158447,\n",
       "  0.23084163665771484,\n",
       "  0.8480196595191956,\n",
       "  0.9623534679412842,\n",
       "  0.11140906810760498,\n",
       "  0.4794517755508423,\n",
       "  0.16576653718948364,\n",
       "  0.18828564882278442,\n",
       "  0.880212128162384,\n",
       "  0.21499043703079224,\n",
       "  0.42415571212768555,\n",
       "  0.3227132558822632,\n",
       "  0.7850984930992126],\n",
       " [0.6708074808120728,\n",
       "  0.7922864556312561,\n",
       "  0.7512170076370239,\n",
       "  0.02341228723526001,\n",
       "  0.2492988109588623,\n",
       "  0.8791328072547913,\n",
       "  0.38929957151412964,\n",
       "  0.3608861565589905,\n",
       "  0.4029330611228943,\n",
       "  0.37594372034072876,\n",
       "  0.5557217001914978,\n",
       "  0.15615159273147583,\n",
       "  0.9795469045639038,\n",
       "  0.18042171001434326,\n",
       "  0.8487274646759033,\n",
       "  0.24315011501312256,\n",
       "  0.45549219846725464,\n",
       "  0.04719102382659912,\n",
       "  0.6856415271759033,\n",
       "  0.22477298974990845,\n",
       "  0.8399365544319153,\n",
       "  0.6178691387176514,\n",
       "  0.5006799697875977,\n",
       "  0.4081278443336487],\n",
       " [0.7766721248626709,\n",
       "  0.23613762855529785,\n",
       "  0.4728872776031494,\n",
       "  0.471790075302124,\n",
       "  0.03420305252075195,\n",
       "  0.567571222782135,\n",
       "  0.05716758966445923,\n",
       "  0.133411705493927,\n",
       "  0.8467495441436768,\n",
       "  0.29922258853912354,\n",
       "  0.5526856780052185,\n",
       "  0.2860743999481201,\n",
       "  0.013825416564941406,\n",
       "  0.902898907661438,\n",
       "  0.2664664387702942,\n",
       "  0.422232985496521,\n",
       "  0.35898810625076294,\n",
       "  0.23859745264053345,\n",
       "  0.6852191090583801,\n",
       "  0.949876070022583,\n",
       "  0.004616260528564453,\n",
       "  0.5802879333496094,\n",
       "  0.907443106174469,\n",
       "  0.1259891390800476],\n",
       " [0.2833822965621948,\n",
       "  0.2334287166595459,\n",
       "  0.7608319520950317,\n",
       "  0.9988902807235718,\n",
       "  0.33298224210739136,\n",
       "  0.4748232364654541,\n",
       "  0.23407942056655884,\n",
       "  0.40662479400634766,\n",
       "  0.1826879382133484,\n",
       "  0.22331684827804565,\n",
       "  0.36956262588500977,\n",
       "  0.3675408363342285,\n",
       "  0.9721652865409851,\n",
       "  0.5720623135566711,\n",
       "  0.09180855751037598,\n",
       "  0.5771411061286926,\n",
       "  0.5564820170402527,\n",
       "  0.47346311807632446,\n",
       "  0.8513066172599792,\n",
       "  0.19392138719558716,\n",
       "  0.1398525834083557,\n",
       "  0.4363119602203369,\n",
       "  0.6086878180503845,\n",
       "  0.3333807587623596],\n",
       " [0.06013846397399902,\n",
       "  0.22427284717559814,\n",
       "  0.33617913722991943,\n",
       "  0.05528604984283447,\n",
       "  0.9956531524658203,\n",
       "  0.3187377452850342,\n",
       "  0.11151885986328125,\n",
       "  0.12974131107330322,\n",
       "  0.47877347469329834,\n",
       "  0.02629023790359497,\n",
       "  0.013588488101959229,\n",
       "  0.9907436370849609,\n",
       "  0.7726516127586365,\n",
       "  0.3112713694572449,\n",
       "  0.8775672316551208,\n",
       "  0.34830302000045776,\n",
       "  0.13643550872802734,\n",
       "  0.8280322551727295,\n",
       "  0.7045931816101074,\n",
       "  0.4547915458679199,\n",
       "  0.9382673501968384,\n",
       "  0.8263896107673645,\n",
       "  0.7668588757514954,\n",
       "  0.11383813619613647],\n",
       " [0.20182007551193237,\n",
       "  0.11842131614685059,\n",
       "  0.03093045949935913,\n",
       "  0.47815126180648804,\n",
       "  0.1366625428199768,\n",
       "  0.25330251455307007,\n",
       "  0.972752034664154,\n",
       "  0.43837404251098633,\n",
       "  0.3885332942008972,\n",
       "  0.6771019697189331,\n",
       "  0.24933010339736938,\n",
       "  0.023930490016937256,\n",
       "  0.307674765586853,\n",
       "  0.07053065299987793,\n",
       "  0.4967474937438965,\n",
       "  0.648440420627594,\n",
       "  0.5424361824989319,\n",
       "  0.8717440962791443,\n",
       "  0.3327009677886963,\n",
       "  0.7262998223304749,\n",
       "  0.23592114448547363,\n",
       "  0.8830875158309937,\n",
       "  0.3550448417663574,\n",
       "  0.8904604911804199],\n",
       " [0.7161503434181213,\n",
       "  0.322304904460907,\n",
       "  0.9886513352394104,\n",
       "  0.8429857492446899,\n",
       "  0.44463902711868286,\n",
       "  0.4317857623100281,\n",
       "  0.2516695261001587,\n",
       "  0.612903356552124,\n",
       "  0.6619264483451843,\n",
       "  0.39250481128692627,\n",
       "  0.8753013014793396,\n",
       "  0.5064316391944885,\n",
       "  0.6706973910331726,\n",
       "  0.6441148519515991,\n",
       "  0.48589271306991577,\n",
       "  0.05274224281311035,\n",
       "  0.519045352935791,\n",
       "  0.16296273469924927,\n",
       "  0.29972022771835327,\n",
       "  0.9460693597793579,\n",
       "  0.18826252222061157,\n",
       "  0.08723610639572144,\n",
       "  0.7084996700286865,\n",
       "  0.9376965165138245],\n",
       " [0.21750497817993164,\n",
       "  0.05851304531097412,\n",
       "  0.2332097887992859,\n",
       "  0.3707966208457947,\n",
       "  0.9578258395195007,\n",
       "  0.4473820924758911,\n",
       "  0.6618911027908325,\n",
       "  0.1892978549003601,\n",
       "  0.7348793148994446,\n",
       "  0.5666335225105286,\n",
       "  0.08572989702224731,\n",
       "  0.9904494285583496,\n",
       "  0.7330604791641235,\n",
       "  0.7410865426063538,\n",
       "  0.47571688890457153,\n",
       "  0.4010471701622009,\n",
       "  0.4022166132926941,\n",
       "  0.8018207550048828,\n",
       "  0.03288376331329346,\n",
       "  0.7225730419158936,\n",
       "  0.2419205904006958,\n",
       "  0.5348281860351562,\n",
       "  0.2828187346458435,\n",
       "  0.39447951316833496],\n",
       " [0.8797000050544739,\n",
       "  0.17388612031936646,\n",
       "  0.5122090578079224,\n",
       "  0.2728937864303589,\n",
       "  0.43036556243896484,\n",
       "  0.709610104560852,\n",
       "  0.9430999755859375,\n",
       "  0.18487322330474854,\n",
       "  0.06823974847793579,\n",
       "  0.6444866061210632,\n",
       "  0.3843876123428345,\n",
       "  0.14283478260040283,\n",
       "  0.7126622200012207,\n",
       "  0.21222549676895142,\n",
       "  0.4083791971206665,\n",
       "  0.6782103180885315,\n",
       "  0.8283113837242126,\n",
       "  0.48409026861190796,\n",
       "  0.38017845153808594,\n",
       "  0.33806324005126953,\n",
       "  0.597602128982544,\n",
       "  0.3684384822845459,\n",
       "  0.5549426674842834,\n",
       "  0.9244885444641113],\n",
       " [0.7301843762397766,\n",
       "  0.3667639493942261,\n",
       "  0.3246801495552063,\n",
       "  0.6167309880256653,\n",
       "  0.7457430362701416,\n",
       "  0.07350671291351318,\n",
       "  0.4574008584022522,\n",
       "  0.05175846815109253,\n",
       "  0.969325065612793,\n",
       "  0.06402873992919922,\n",
       "  0.4488140940666199,\n",
       "  0.7256600260734558,\n",
       "  0.17875635623931885,\n",
       "  0.6729238033294678,\n",
       "  0.9148029088973999,\n",
       "  0.21379029750823975,\n",
       "  0.4398112893104553,\n",
       "  0.45118969678878784,\n",
       "  0.14171558618545532,\n",
       "  0.5658754110336304,\n",
       "  0.7254167795181274,\n",
       "  0.11544090509414673,\n",
       "  0.4014333486557007,\n",
       "  0.1376047134399414],\n",
       " [0.8627140522003174,\n",
       "  0.43404895067214966,\n",
       "  0.5953429341316223,\n",
       "  0.5906013250350952,\n",
       "  0.3833118677139282,\n",
       "  0.06037360429763794,\n",
       "  0.321505069732666,\n",
       "  0.5640701055526733,\n",
       "  0.03828352689743042,\n",
       "  0.9771932363510132,\n",
       "  0.07083934545516968,\n",
       "  0.32478946447372437,\n",
       "  0.6461970806121826,\n",
       "  0.32387393712997437,\n",
       "  0.4829638600349426,\n",
       "  0.8176276087760925,\n",
       "  0.4967184066772461,\n",
       "  0.6689603924751282,\n",
       "  0.41783493757247925,\n",
       "  0.30603569746017456,\n",
       "  0.584978461265564,\n",
       "  0.241493821144104,\n",
       "  0.6950649619102478,\n",
       "  0.8818369507789612],\n",
       " [0.5278763175010681,\n",
       "  0.2873169183731079,\n",
       "  0.2818722128868103,\n",
       "  0.2520172595977783,\n",
       "  0.4176732897758484,\n",
       "  0.0763312578201294,\n",
       "  0.4948490858078003,\n",
       "  0.7980554699897766,\n",
       "  0.41293203830718994,\n",
       "  0.6247667670249939,\n",
       "  0.42429786920547485,\n",
       "  0.9954981803894043,\n",
       "  0.06813597679138184,\n",
       "  0.6470561027526855,\n",
       "  0.9265364408493042,\n",
       "  0.49042755365371704,\n",
       "  0.864108145236969,\n",
       "  0.8722209334373474,\n",
       "  0.32743990421295166,\n",
       "  0.7728254795074463,\n",
       "  0.06080693006515503,\n",
       "  0.8797509074211121,\n",
       "  0.0024796128273010254,\n",
       "  0.42337238788604736],\n",
       " [0.8347588777542114,\n",
       "  0.2618999481201172,\n",
       "  0.40214812755584717,\n",
       "  0.2648565173149109,\n",
       "  0.05426609516143799,\n",
       "  0.6673035621643066,\n",
       "  0.2864255905151367,\n",
       "  0.39264339208602905,\n",
       "  0.2987397313117981,\n",
       "  0.1962164044380188,\n",
       "  0.42000842094421387,\n",
       "  0.5934136509895325,\n",
       "  0.7115191221237183,\n",
       "  0.3533552885055542,\n",
       "  0.3381151556968689,\n",
       "  0.956283688545227,\n",
       "  0.05840480327606201,\n",
       "  0.6154913902282715,\n",
       "  0.21958762407302856,\n",
       "  0.7068090438842773,\n",
       "  0.1489393711090088,\n",
       "  0.9389822483062744,\n",
       "  0.010618269443511963,\n",
       "  0.7891921997070312],\n",
       " [0.9031625986099243,\n",
       "  0.5746846199035645,\n",
       "  0.7374333143234253,\n",
       "  0.7505376935005188,\n",
       "  0.9929342865943909,\n",
       "  0.16042625904083252,\n",
       "  0.8214770555496216,\n",
       "  0.9529294967651367,\n",
       "  0.9856547713279724,\n",
       "  0.3633022904396057,\n",
       "  0.32339024543762207,\n",
       "  0.30128657817840576,\n",
       "  0.3039305806159973,\n",
       "  0.8830103278160095,\n",
       "  0.37979429960250854,\n",
       "  0.9356650114059448,\n",
       "  0.11335235834121704,\n",
       "  0.8428447246551514,\n",
       "  0.6909417510032654,\n",
       "  0.023832857608795166,\n",
       "  0.7272129058837891,\n",
       "  0.04239684343338013,\n",
       "  0.8012614250183105,\n",
       "  0.3951230049133301],\n",
       " [0.5404825806617737,\n",
       "  0.9982209205627441,\n",
       "  0.543380856513977,\n",
       "  0.8962242603302002,\n",
       "  0.22476476430892944,\n",
       "  0.4636862277984619,\n",
       "  0.42296016216278076,\n",
       "  0.8237032890319824,\n",
       "  0.2508993148803711,\n",
       "  0.5736754536628723,\n",
       "  0.8104787468910217,\n",
       "  0.4678729176521301,\n",
       "  0.1858488917350769,\n",
       "  0.2281714677810669,\n",
       "  0.08445072174072266,\n",
       "  0.44231247901916504,\n",
       "  0.7230706810951233,\n",
       "  0.6884338855743408,\n",
       "  0.528298556804657,\n",
       "  0.912007212638855,\n",
       "  0.4214939475059509,\n",
       "  0.2523161768913269,\n",
       "  0.25037986040115356,\n",
       "  0.9388822317123413],\n",
       " [0.5108742713928223,\n",
       "  0.41192495822906494,\n",
       "  0.34163790941238403,\n",
       "  0.9518628120422363,\n",
       "  0.021038413047790527,\n",
       "  0.012830913066864014,\n",
       "  0.4677315354347229,\n",
       "  0.15331655740737915,\n",
       "  0.3841785788536072,\n",
       "  0.3521420359611511,\n",
       "  0.06431907415390015,\n",
       "  0.8575318455696106,\n",
       "  0.7605745792388916,\n",
       "  0.9467661380767822,\n",
       "  0.9285450577735901,\n",
       "  0.2766888737678528,\n",
       "  0.747113823890686,\n",
       "  0.9495612978935242,\n",
       "  0.14270347356796265,\n",
       "  0.5295022130012512,\n",
       "  0.7025392651557922,\n",
       "  0.36735475063323975,\n",
       "  0.4701181650161743,\n",
       "  0.17830222845077515],\n",
       " [0.3407300114631653,\n",
       "  0.6193392872810364,\n",
       "  0.0475650429725647,\n",
       "  0.26717835664749146,\n",
       "  0.5347641706466675,\n",
       "  0.8229882121086121,\n",
       "  0.5454607605934143,\n",
       "  0.4499441385269165,\n",
       "  0.47416114807128906,\n",
       "  0.1557680368423462,\n",
       "  0.3045879602432251,\n",
       "  0.9732917547225952,\n",
       "  0.5793803930282593,\n",
       "  0.41454988718032837,\n",
       "  0.10170257091522217,\n",
       "  0.9749823808670044,\n",
       "  0.5374884009361267,\n",
       "  0.4057123064994812,\n",
       "  0.17650359869003296,\n",
       "  0.8375877737998962,\n",
       "  0.8999061584472656,\n",
       "  0.7294188737869263,\n",
       "  0.12503331899642944,\n",
       "  0.18966251611709595],\n",
       " [0.30768853425979614,\n",
       "  0.7384475469589233,\n",
       "  0.015505492687225342,\n",
       "  0.6124745607376099,\n",
       "  0.2754164934158325,\n",
       "  0.26438575983047485,\n",
       "  0.10762578248977661,\n",
       "  0.11529338359832764,\n",
       "  0.8399868607521057,\n",
       "  0.0983080267906189,\n",
       "  0.17567497491836548,\n",
       "  0.6601385474205017,\n",
       "  0.3813897967338562,\n",
       "  0.31370997428894043,\n",
       "  0.8438876867294312,\n",
       "  0.760908305644989,\n",
       "  0.09656882286071777,\n",
       "  0.6305789351463318,\n",
       "  0.7717180848121643,\n",
       "  0.27529335021972656,\n",
       "  0.07170194387435913,\n",
       "  0.1978778839111328,\n",
       "  0.9695673584938049,\n",
       "  0.6807472109794617],\n",
       " [0.6820734739303589,\n",
       "  0.3296283483505249,\n",
       "  0.929691731929779,\n",
       "  0.4719504117965698,\n",
       "  0.4720638394355774,\n",
       "  0.8148122429847717,\n",
       "  0.38946104049682617,\n",
       "  0.13920944929122925,\n",
       "  0.3999266028404236,\n",
       "  0.9965012669563293,\n",
       "  0.03151559829711914,\n",
       "  0.683695375919342,\n",
       "  0.2629810571670532,\n",
       "  0.5920589566230774,\n",
       "  0.2996246814727783,\n",
       "  0.7622617483139038,\n",
       "  0.16636669635772705,\n",
       "  0.8613442182540894,\n",
       "  0.37052619457244873,\n",
       "  0.9572741389274597,\n",
       "  0.6941325068473816,\n",
       "  0.12122780084609985,\n",
       "  0.24593818187713623,\n",
       "  0.8280490636825562],\n",
       " [0.9371556043624878,\n",
       "  0.7542567253112793,\n",
       "  0.9308524131774902,\n",
       "  0.3568161129951477,\n",
       "  0.6175740957260132,\n",
       "  0.1954329013824463,\n",
       "  0.16908174753189087,\n",
       "  0.21490401029586792,\n",
       "  0.23303478956222534,\n",
       "  0.3989086151123047,\n",
       "  0.5768462419509888,\n",
       "  0.5319854617118835,\n",
       "  0.7363356947898865,\n",
       "  0.45761239528656006,\n",
       "  0.7082576751708984,\n",
       "  0.27395617961883545,\n",
       "  0.37607908248901367,\n",
       "  0.9095130562782288,\n",
       "  0.77394700050354,\n",
       "  0.5109514594078064,\n",
       "  0.44006073474884033,\n",
       "  0.6285182237625122,\n",
       "  0.2616862654685974,\n",
       "  0.0017880797386169434],\n",
       " [0.959611177444458,\n",
       "  0.5030536651611328,\n",
       "  0.30129337310791016,\n",
       "  0.2564029097557068,\n",
       "  0.008641660213470459,\n",
       "  0.4910498261451721,\n",
       "  0.583222508430481,\n",
       "  0.5573387742042542,\n",
       "  0.6468589305877686,\n",
       "  0.9469466209411621,\n",
       "  0.2881196141242981,\n",
       "  0.6409218311309814,\n",
       "  0.8361781239509583,\n",
       "  0.28177350759506226,\n",
       "  0.9233126044273376,\n",
       "  0.2853601574897766,\n",
       "  0.7532985806465149,\n",
       "  0.7375539541244507,\n",
       "  0.4353417158126831,\n",
       "  0.42532598972320557,\n",
       "  0.6114064455032349,\n",
       "  0.3754226565361023,\n",
       "  0.4073583483695984,\n",
       "  0.1441829800605774],\n",
       " [0.7235710024833679,\n",
       "  0.5947197675704956,\n",
       "  0.7116502523422241,\n",
       "  0.6465832591056824,\n",
       "  0.603962242603302,\n",
       "  0.9792604446411133,\n",
       "  0.8039786219596863,\n",
       "  0.6985043883323669,\n",
       "  0.5886170864105225,\n",
       "  0.723453938961029,\n",
       "  0.8108099102973938,\n",
       "  0.6042496562004089,\n",
       "  0.34363681077957153,\n",
       "  0.07281911373138428,\n",
       "  0.1700228452682495,\n",
       "  0.9344823956489563,\n",
       "  0.08240348100662231,\n",
       "  0.31254297494888306,\n",
       "  0.43593114614486694,\n",
       "  0.32732874155044556,\n",
       "  0.40100598335266113,\n",
       "  0.5097236633300781,\n",
       "  0.027260899543762207,\n",
       "  0.8049864768981934],\n",
       " [0.47277700901031494,\n",
       "  0.6791255474090576,\n",
       "  0.10780620574951172,\n",
       "  0.1122627854347229,\n",
       "  0.02433168888092041,\n",
       "  0.330624520778656,\n",
       "  0.4789966344833374,\n",
       "  0.4015223979949951,\n",
       "  0.17080432176589966,\n",
       "  0.7604144215583801,\n",
       "  0.5211796164512634,\n",
       "  0.7280527353286743,\n",
       "  0.6388103365898132,\n",
       "  0.5968531370162964,\n",
       "  0.49665367603302,\n",
       "  0.1659032106399536,\n",
       "  0.6172025203704834,\n",
       "  0.22284150123596191,\n",
       "  0.3888063430786133,\n",
       "  0.7905269265174866,\n",
       "  0.7062603235244751,\n",
       "  0.9073336720466614,\n",
       "  0.40300238132476807,\n",
       "  0.7752321362495422],\n",
       " [0.6192333102226257,\n",
       "  0.6370730400085449,\n",
       "  0.7485200762748718,\n",
       "  0.287470281124115,\n",
       "  0.5166627168655396,\n",
       "  0.10006511211395264,\n",
       "  0.188232421875,\n",
       "  0.03795671463012695,\n",
       "  0.7020978927612305,\n",
       "  0.25785690546035767,\n",
       "  0.31462836265563965,\n",
       "  0.4354298710823059,\n",
       "  0.9249745011329651,\n",
       "  0.3554535508155823,\n",
       "  0.8831756114959717,\n",
       "  0.5225762128829956,\n",
       "  0.6011112332344055,\n",
       "  0.8235213160514832,\n",
       "  0.1693097949028015,\n",
       "  0.6545971035957336,\n",
       "  0.4148685336112976,\n",
       "  0.36934274435043335,\n",
       "  0.21428126096725464,\n",
       "  0.8341085910797119],\n",
       " [0.869868814945221,\n",
       "  0.46720385551452637,\n",
       "  0.22879564762115479,\n",
       "  0.2773452401161194,\n",
       "  0.403831422328949,\n",
       "  0.37629562616348267,\n",
       "  0.7579703330993652,\n",
       "  0.45576584339141846,\n",
       "  0.2474212646484375,\n",
       "  0.81410151720047,\n",
       "  0.11769688129425049,\n",
       "  0.2665184736251831,\n",
       "  0.1928425431251526,\n",
       "  0.8797273635864258,\n",
       "  0.2802128195762634,\n",
       "  0.9649653434753418,\n",
       "  0.06826430559158325,\n",
       "  0.7477635741233826,\n",
       "  0.19770735502243042,\n",
       "  0.14957034587860107,\n",
       "  0.7632057666778564,\n",
       "  0.9613401889801025,\n",
       "  0.5018313527107239,\n",
       "  0.8073033690452576],\n",
       " [0.7668329477310181,\n",
       "  0.3303513526916504,\n",
       "  0.6718864440917969,\n",
       "  0.48857372999191284,\n",
       "  0.32136690616607666,\n",
       "  0.4832664132118225,\n",
       "  0.6269915699958801,\n",
       "  0.2339949607849121,\n",
       "  0.14030921459197998,\n",
       "  0.7047443985939026,\n",
       "  0.49281638860702515,\n",
       "  0.9941434860229492,\n",
       "  0.9361689686775208,\n",
       "  0.16434091329574585,\n",
       "  0.038116395473480225,\n",
       "  0.9638723134994507,\n",
       "  0.5319609642028809,\n",
       "  0.4094235301017761,\n",
       "  0.23000770807266235,\n",
       "  0.8444132804870605,\n",
       "  0.7363225221633911,\n",
       "  0.32251662015914917,\n",
       "  0.05674111843109131,\n",
       "  0.023492395877838135],\n",
       " [0.1657041311264038,\n",
       "  0.4702160954475403,\n",
       "  0.1088445782661438,\n",
       "  0.534086287021637,\n",
       "  0.9660806059837341,\n",
       "  0.881056010723114,\n",
       "  0.07511401176452637,\n",
       "  0.756440281867981,\n",
       "  0.23709636926651,\n",
       "  0.33542847633361816,\n",
       "  0.3032262921333313,\n",
       "  0.1934816837310791,\n",
       "  0.6850771903991699,\n",
       "  0.2765752077102661,\n",
       "  0.659683883190155,\n",
       "  0.8021752238273621,\n",
       "  0.6868954300880432,\n",
       "  0.5015074014663696,\n",
       "  0.36836767196655273,\n",
       "  0.28495490550994873,\n",
       "  0.34151583909988403,\n",
       "  0.43741369247436523,\n",
       "  0.43776190280914307,\n",
       "  0.5386466979980469],\n",
       " [0.8168154358863831,\n",
       "  0.25619983673095703,\n",
       "  0.8262196779251099,\n",
       "  0.4268531799316406,\n",
       "  0.4830546975135803,\n",
       "  0.5843554139137268,\n",
       "  0.7469015717506409,\n",
       "  0.8021708726882935,\n",
       "  0.7383943796157837,\n",
       "  0.649308443069458,\n",
       "  0.55063396692276,\n",
       "  0.3061959147453308,\n",
       "  0.8984470367431641,\n",
       "  0.2697928547859192,\n",
       "  0.44336795806884766,\n",
       "  0.17953342199325562,\n",
       "  0.25586074590682983,\n",
       "  0.8391982913017273,\n",
       "  0.3659871816635132,\n",
       "  0.0624578595161438,\n",
       "  0.4683834910392761,\n",
       "  0.7666265368461609,\n",
       "  0.2757663130760193,\n",
       "  0.18237006664276123],\n",
       " [0.295616090297699,\n",
       "  0.21266502141952515,\n",
       "  0.2468886375427246,\n",
       "  0.8932621479034424,\n",
       "  0.057698071002960205,\n",
       "  0.22548210620880127,\n",
       "  0.8153427243232727,\n",
       "  0.1866949200630188,\n",
       "  0.5360703468322754,\n",
       "  0.8070114254951477,\n",
       "  0.22067278623580933,\n",
       "  0.8421688675880432,\n",
       "  0.48162949085235596,\n",
       "  0.8387053608894348,\n",
       "  0.8585330247879028,\n",
       "  0.46937745809555054,\n",
       "  0.35885077714920044,\n",
       "  0.7202326655387878,\n",
       "  0.9633497595787048,\n",
       "  0.7005031704902649,\n",
       "  0.5411510467529297,\n",
       "  0.615943193435669,\n",
       "  0.9749361872673035,\n",
       "  0.6105771064758301],\n",
       " [0.5817846655845642,\n",
       "  0.7176246643066406,\n",
       "  0.2498319149017334,\n",
       "  0.7335708737373352,\n",
       "  0.6860873699188232,\n",
       "  0.18947303295135498,\n",
       "  0.3801996111869812,\n",
       "  0.9291592240333557,\n",
       "  0.11349481344223022,\n",
       "  0.5523048639297485,\n",
       "  0.8656185865402222,\n",
       "  0.6288904547691345,\n",
       "  0.9227899312973022,\n",
       "  0.3255232572555542,\n",
       "  0.6162500977516174,\n",
       "  0.34798794984817505,\n",
       "  0.234305739402771,\n",
       "  0.30118054151535034,\n",
       "  0.7103544473648071,\n",
       "  0.6725784540176392,\n",
       "  0.5100056529045105,\n",
       "  0.5939355492591858,\n",
       "  0.727694034576416,\n",
       "  0.14420545101165771],\n",
       " [0.4298258423805237,\n",
       "  0.14458680152893066,\n",
       "  0.3130478262901306,\n",
       "  0.4884054660797119,\n",
       "  0.6868376135826111,\n",
       "  0.07150375843048096,\n",
       "  0.5399500131607056,\n",
       "  0.15686190128326416,\n",
       "  0.7752780914306641,\n",
       "  0.9616037011146545,\n",
       "  0.6524850130081177,\n",
       "  0.060240745544433594,\n",
       "  0.15553408861160278,\n",
       "  0.019816935062408447,\n",
       "  0.5381185412406921,\n",
       "  0.21743881702423096,\n",
       "  0.17632347345352173,\n",
       "  0.05091047286987305,\n",
       "  0.5075255036354065,\n",
       "  0.10915589332580566,\n",
       "  0.894188404083252,\n",
       "  0.21473056077957153,\n",
       "  0.55296391248703,\n",
       "  0.6916670203208923],\n",
       " [0.9850078225135803,\n",
       "  0.8478644490242004,\n",
       "  0.49122142791748047,\n",
       "  0.6337378025054932,\n",
       "  0.5349169373512268,\n",
       "  0.5604636073112488,\n",
       "  0.870154857635498,\n",
       "  0.6610506772994995,\n",
       "  0.7893663048744202,\n",
       "  0.42241495847702026,\n",
       "  0.5484933257102966,\n",
       "  0.40162742137908936,\n",
       "  0.17548227310180664,\n",
       "  0.32358598709106445,\n",
       "  0.9223341941833496,\n",
       "  0.14570516347885132,\n",
       "  0.6924039125442505,\n",
       "  0.8514766097068787,\n",
       "  0.8487749099731445,\n",
       "  0.8606956005096436,\n",
       "  0.3075786232948303,\n",
       "  0.3080832362174988,\n",
       "  0.2649853229522705,\n",
       "  0.9559140801429749],\n",
       " [0.28475815057754517,\n",
       "  0.45674222707748413,\n",
       "  0.6928941011428833,\n",
       "  0.7371456623077393,\n",
       "  0.07066738605499268,\n",
       "  0.5016458034515381,\n",
       "  0.3039567470550537,\n",
       "  0.4154239296913147,\n",
       "  0.1580352783203125,\n",
       "  0.48441213369369507,\n",
       "  0.05752748250961304,\n",
       "  0.3537134528160095,\n",
       "  0.014119505882263184,\n",
       "  0.02464771270751953,\n",
       "  0.6086936593055725,\n",
       "  0.4219224452972412,\n",
       "  0.35039258003234863,\n",
       "  0.5590742230415344,\n",
       "  0.07956439256668091,\n",
       "  0.6067834496498108,\n",
       "  0.037780582904815674,\n",
       "  0.1396065354347229,\n",
       "  0.6472206115722656,\n",
       "  0.32027941942214966],\n",
       " [0.3333330750465393,\n",
       "  0.9181452393531799,\n",
       "  0.18240618705749512,\n",
       "  0.5707316398620605,\n",
       "  0.018243849277496338,\n",
       "  0.9933673739433289,\n",
       "  0.9261890053749084,\n",
       "  0.9850849509239197,\n",
       "  0.1708781123161316,\n",
       "  0.21155661344528198,\n",
       "  0.4600728750228882,\n",
       "  0.579104483127594,\n",
       "  0.8496964573860168,\n",
       "  0.9527456760406494,\n",
       "  0.5258225798606873,\n",
       "  0.7486148476600647,\n",
       "  0.4407522678375244,\n",
       "  0.9077316522598267,\n",
       "  0.30222153663635254,\n",
       "  0.12169498205184937,\n",
       "  0.7950178980827332,\n",
       "  0.09572434425354004,\n",
       "  0.2912048101425171,\n",
       "  0.40093863010406494],\n",
       " [0.014370083808898926,\n",
       "  0.8817256689071655,\n",
       "  0.20622342824935913,\n",
       "  0.7986721396446228,\n",
       "  0.692841649055481,\n",
       "  0.3638375997543335,\n",
       "  0.5950441360473633,\n",
       "  0.3641364574432373,\n",
       "  0.4681863784790039,\n",
       "  0.2995537519454956,\n",
       "  0.2157776951789856,\n",
       "  0.609394907951355,\n",
       "  0.339084267616272,\n",
       "  0.7116211652755737,\n",
       "  0.28043603897094727,\n",
       "  0.4468623995780945,\n",
       "  0.7884249687194824,\n",
       "  0.6327028274536133,\n",
       "  0.556289553642273,\n",
       "  0.7047125697135925,\n",
       "  0.3069230914115906,\n",
       "  0.9584479331970215,\n",
       "  0.30665236711502075,\n",
       "  0.36205005645751953],\n",
       " [0.08796906471252441,\n",
       "  0.18813735246658325,\n",
       "  0.890485942363739,\n",
       "  0.6036415100097656,\n",
       "  0.7284020781517029,\n",
       "  0.19319301843643188,\n",
       "  0.32465118169784546,\n",
       "  0.012488007545471191,\n",
       "  0.811217188835144,\n",
       "  0.2631301283836365,\n",
       "  0.8752948641777039,\n",
       "  0.660155177116394,\n",
       "  0.48723161220550537,\n",
       "  0.35815542936325073,\n",
       "  0.8434664607048035,\n",
       "  0.9925002455711365,\n",
       "  0.010786592960357666,\n",
       "  0.7297167181968689,\n",
       "  0.3753010034561157,\n",
       "  0.9023096561431885,\n",
       "  0.03257298469543457,\n",
       "  0.2203301191329956,\n",
       "  0.6183492541313171,\n",
       "  0.20541226863861084],\n",
       " [0.6873410940170288,\n",
       "  0.15189605951309204,\n",
       "  0.33179450035095215,\n",
       "  0.2864665985107422,\n",
       "  0.7932015657424927,\n",
       "  0.35195392370224,\n",
       "  0.4172806143760681,\n",
       "  0.4589903950691223,\n",
       "  0.16622334718704224,\n",
       "  0.09933573007583618,\n",
       "  0.4550555944442749,\n",
       "  0.18379813432693481,\n",
       "  0.6698452234268188,\n",
       "  0.5712031722068787,\n",
       "  0.9025757312774658,\n",
       "  0.08364039659500122,\n",
       "  0.23849964141845703,\n",
       "  0.6338733434677124,\n",
       "  0.8745152354240417,\n",
       "  0.11651057004928589,\n",
       "  0.6084762215614319,\n",
       "  0.5160244703292847,\n",
       "  0.5891624093055725,\n",
       "  0.9245777726173401],\n",
       " [0.42366039752960205,\n",
       "  0.8325973749160767,\n",
       "  0.18336039781570435,\n",
       "  0.3160848021507263,\n",
       "  0.7272903323173523,\n",
       "  0.42520153522491455,\n",
       "  0.6536012291908264,\n",
       "  0.2917960286140442,\n",
       "  0.02253502607345581,\n",
       "  0.14347350597381592,\n",
       "  0.04150032997131348,\n",
       "  0.7913700342178345,\n",
       "  0.7774111032485962,\n",
       "  0.49791353940963745,\n",
       "  0.5282720923423767,\n",
       "  0.21428543329238892,\n",
       "  0.7982823252677917,\n",
       "  0.6784532070159912,\n",
       "  0.9601502418518066,\n",
       "  0.15053218603134155,\n",
       "  0.9002412557601929,\n",
       "  0.5931301712989807,\n",
       "  0.969331681728363,\n",
       "  0.4737358093261719]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensy_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58280866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensy_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c42ea38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_list = []\n",
    "\n",
    "for sublist in tensy_l:\n",
    "    \n",
    "    tmp_list = [list((sublist[i], sublist[i+1])) for i in range(0, len(sublist) - 1, 2)]\n",
    "    batch_list.append(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48293c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.17168915271759033, 0.302817165851593],\n",
       "  [0.23487138748168945, 0.11001861095428467],\n",
       "  [0.19454807043075562, 0.9708924293518066],\n",
       "  [0.8996075987815857, 0.8302733898162842],\n",
       "  [0.5763616561889648, 0.6052727103233337],\n",
       "  [0.20066696405410767, 0.24728244543075562],\n",
       "  [0.7759718894958496, 0.706483006477356],\n",
       "  [0.19881552457809448, 0.3861386179924011],\n",
       "  [0.7213626503944397, 0.6232663989067078],\n",
       "  [0.36151885986328125, 0.5948505401611328],\n",
       "  [0.7508516311645508, 0.2725784182548523],\n",
       "  [0.026015818119049072, 0.6215766668319702]],\n",
       " [[0.02072244882583618, 0.4277816414833069],\n",
       "  [0.42437058687210083, 0.9133836030960083],\n",
       "  [0.894695520401001, 0.40514326095581055],\n",
       "  [0.12138873338699341, 0.03523838520050049],\n",
       "  [0.637154757976532, 0.1049647331237793],\n",
       "  [0.17650604248046875, 0.5680537223815918],\n",
       "  [0.5146389603614807, 0.7100184559822083],\n",
       "  [0.6107532382011414, 0.5055559873580933],\n",
       "  [0.3960137963294983, 0.03858530521392822],\n",
       "  [0.3522071838378906, 0.06991809606552124],\n",
       "  [0.7755093574523926, 0.863541841506958],\n",
       "  [0.23714834451675415, 0.8501376509666443]],\n",
       " [[0.12422841787338257, 0.7724658846855164],\n",
       "  [0.606565535068512, 0.32325422763824463],\n",
       "  [0.3566409945487976, 0.5438502430915833],\n",
       "  [0.7787052989006042, 0.2408994436264038],\n",
       "  [0.6985463500022888, 0.8152540922164917],\n",
       "  [0.6523404121398926, 0.3795153498649597],\n",
       "  [0.9208999276161194, 0.3006328344345093],\n",
       "  [0.853693425655365, 0.7555317878723145],\n",
       "  [0.4046822786331177, 0.975095272064209],\n",
       "  [0.934761106967926, 0.4659804701805115],\n",
       "  [0.5050164461135864, 0.6474541425704956],\n",
       "  [0.2170504331588745, 0.166692852973938]],\n",
       " [[0.24472099542617798, 0.4014214277267456],\n",
       "  [0.13364601135253906, 0.47675299644470215],\n",
       "  [0.5576980710029602, 0.35181862115859985],\n",
       "  [0.9024598002433777, 0.0974850058555603],\n",
       "  [0.34284019470214844, 0.6522940993309021],\n",
       "  [0.2156085968017578, 0.16868728399276733],\n",
       "  [0.7325751185417175, 0.35382795333862305],\n",
       "  [0.38057106733322144, 0.4929543733596802],\n",
       "  [0.013283312320709229, 0.10836178064346313],\n",
       "  [0.6712048649787903, 0.10221725702285767],\n",
       "  [0.8894752264022827, 0.767790675163269],\n",
       "  [0.6004040837287903, 0.04429209232330322]],\n",
       " [[0.7637056112289429, 0.7655920386314392],\n",
       "  [0.9840447306632996, 0.7797181010246277],\n",
       "  [0.30164825916290283, 0.5220595598220825],\n",
       "  [0.6425966024398804, 0.1576417088508606],\n",
       "  [0.9643210172653198, 0.455949604511261],\n",
       "  [0.23938852548599243, 0.8483310341835022],\n",
       "  [0.497348427772522, 0.32600241899490356],\n",
       "  [0.9982336163520813, 0.9611243605613708],\n",
       "  [0.636556088924408, 0.7490946054458618],\n",
       "  [0.2319687008857727, 0.059793829917907715],\n",
       "  [0.7296817898750305, 0.38178062438964844],\n",
       "  [0.025968611240386963, 0.8931936025619507]],\n",
       " [[0.40517616271972656, 0.1458902359008789],\n",
       "  [0.023820579051971436, 0.6724759936332703],\n",
       "  [0.5676811337471008, 0.5715808868408203],\n",
       "  [0.3215993046760559, 0.7823853492736816],\n",
       "  [0.21731233596801758, 0.2605699896812439],\n",
       "  [0.3096277117729187, 0.33651208877563477],\n",
       "  [0.8131352066993713, 0.09882962703704834],\n",
       "  [0.09538155794143677, 0.7711284756660461],\n",
       "  [0.4023793935775757, 0.11439526081085205],\n",
       "  [0.2173967957496643, 0.5438826680183411],\n",
       "  [0.308529257774353, 0.1362897753715515],\n",
       "  [0.0737835168838501, 0.3400459289550781]],\n",
       " [[0.6102308630943298, 0.6759757995605469],\n",
       "  [0.8965817093849182, 0.8177597522735596],\n",
       "  [0.4744412302970886, 0.5801161527633667],\n",
       "  [0.22408944368362427, 0.42200952768325806],\n",
       "  [0.7262744307518005, 0.02270972728729248],\n",
       "  [0.2617708444595337, 0.992360532283783],\n",
       "  [0.8091744184494019, 0.7018395662307739],\n",
       "  [0.8307307958602905, 0.9890048503875732],\n",
       "  [0.22597986459732056, 0.4836966395378113],\n",
       "  [0.16138458251953125, 0.5748518705368042],\n",
       "  [0.33556467294692993, 0.2654010057449341],\n",
       "  [0.4849807024002075, 0.41523969173431396]],\n",
       " [[0.350439190864563, 0.20154917240142822],\n",
       "  [0.30837082862854004, 0.9825896620750427],\n",
       "  [0.7057996392250061, 0.36860567331314087],\n",
       "  [0.1850232481956482, 0.1129578948020935],\n",
       "  [0.5790365934371948, 0.597599983215332],\n",
       "  [0.7316299080848694, 0.19119524955749512],\n",
       "  [0.7100183963775635, 0.854689359664917],\n",
       "  [0.019701838493347168, 0.3797488808631897],\n",
       "  [0.8321186900138855, 0.9242724776268005],\n",
       "  [0.6883131265640259, 0.21801412105560303],\n",
       "  [0.4232826232910156, 0.9737843871116638],\n",
       "  [0.4597240090370178, 0.7678506970405579]],\n",
       " [[0.20535939931869507, 0.11009126901626587],\n",
       "  [0.07486522197723389, 0.09403306245803833],\n",
       "  [0.27560514211654663, 0.9378510117530823],\n",
       "  [0.7461086511611938, 0.04273957014083862],\n",
       "  [0.921593427658081, 0.343855619430542],\n",
       "  [0.26891666650772095, 0.5061367154121399],\n",
       "  [0.35598331689834595, 0.29854822158813477],\n",
       "  [0.017540931701660156, 0.5519097447395325],\n",
       "  [0.5086615085601807, 0.7562202215194702],\n",
       "  [0.4178076386451721, 0.22784340381622314],\n",
       "  [0.6288447976112366, 0.9317689538002014],\n",
       "  [0.9463105201721191, 0.8811867237091064]],\n",
       " [[0.2663917541503906, 0.6182193756103516],\n",
       "  [0.8131062388420105, 0.596411406993866],\n",
       "  [0.8423597812652588, 0.021668434143066406],\n",
       "  [0.7566080093383789, 0.33213353157043457],\n",
       "  [0.5247715711593628, 0.9161971807479858],\n",
       "  [0.5173883438110352, 0.45366519689559937],\n",
       "  [0.1814236044883728, 0.22559189796447754],\n",
       "  [0.5095747709274292, 0.549243152141571],\n",
       "  [0.11840486526489258, 0.8841248154640198],\n",
       "  [0.07628554105758667, 0.15482968091964722],\n",
       "  [0.816685140132904, 0.6827566623687744],\n",
       "  [0.6540245413780212, 0.1532222032546997]],\n",
       " [[0.5561574101448059, 0.7712180614471436],\n",
       "  [0.3210383653640747, 0.13564151525497437],\n",
       "  [0.2535169720649719, 0.05336874723434448],\n",
       "  [0.43679171800613403, 0.8281631469726562],\n",
       "  [0.957253098487854, 0.5046791434288025],\n",
       "  [0.10412168502807617, 0.19023478031158447],\n",
       "  [0.23084163665771484, 0.8480196595191956],\n",
       "  [0.9623534679412842, 0.11140906810760498],\n",
       "  [0.4794517755508423, 0.16576653718948364],\n",
       "  [0.18828564882278442, 0.880212128162384],\n",
       "  [0.21499043703079224, 0.42415571212768555],\n",
       "  [0.3227132558822632, 0.7850984930992126]],\n",
       " [[0.6708074808120728, 0.7922864556312561],\n",
       "  [0.7512170076370239, 0.02341228723526001],\n",
       "  [0.2492988109588623, 0.8791328072547913],\n",
       "  [0.38929957151412964, 0.3608861565589905],\n",
       "  [0.4029330611228943, 0.37594372034072876],\n",
       "  [0.5557217001914978, 0.15615159273147583],\n",
       "  [0.9795469045639038, 0.18042171001434326],\n",
       "  [0.8487274646759033, 0.24315011501312256],\n",
       "  [0.45549219846725464, 0.04719102382659912],\n",
       "  [0.6856415271759033, 0.22477298974990845],\n",
       "  [0.8399365544319153, 0.6178691387176514],\n",
       "  [0.5006799697875977, 0.4081278443336487]],\n",
       " [[0.7766721248626709, 0.23613762855529785],\n",
       "  [0.4728872776031494, 0.471790075302124],\n",
       "  [0.03420305252075195, 0.567571222782135],\n",
       "  [0.05716758966445923, 0.133411705493927],\n",
       "  [0.8467495441436768, 0.29922258853912354],\n",
       "  [0.5526856780052185, 0.2860743999481201],\n",
       "  [0.013825416564941406, 0.902898907661438],\n",
       "  [0.2664664387702942, 0.422232985496521],\n",
       "  [0.35898810625076294, 0.23859745264053345],\n",
       "  [0.6852191090583801, 0.949876070022583],\n",
       "  [0.004616260528564453, 0.5802879333496094],\n",
       "  [0.907443106174469, 0.1259891390800476]],\n",
       " [[0.2833822965621948, 0.2334287166595459],\n",
       "  [0.7608319520950317, 0.9988902807235718],\n",
       "  [0.33298224210739136, 0.4748232364654541],\n",
       "  [0.23407942056655884, 0.40662479400634766],\n",
       "  [0.1826879382133484, 0.22331684827804565],\n",
       "  [0.36956262588500977, 0.3675408363342285],\n",
       "  [0.9721652865409851, 0.5720623135566711],\n",
       "  [0.09180855751037598, 0.5771411061286926],\n",
       "  [0.5564820170402527, 0.47346311807632446],\n",
       "  [0.8513066172599792, 0.19392138719558716],\n",
       "  [0.1398525834083557, 0.4363119602203369],\n",
       "  [0.6086878180503845, 0.3333807587623596]],\n",
       " [[0.06013846397399902, 0.22427284717559814],\n",
       "  [0.33617913722991943, 0.05528604984283447],\n",
       "  [0.9956531524658203, 0.3187377452850342],\n",
       "  [0.11151885986328125, 0.12974131107330322],\n",
       "  [0.47877347469329834, 0.02629023790359497],\n",
       "  [0.013588488101959229, 0.9907436370849609],\n",
       "  [0.7726516127586365, 0.3112713694572449],\n",
       "  [0.8775672316551208, 0.34830302000045776],\n",
       "  [0.13643550872802734, 0.8280322551727295],\n",
       "  [0.7045931816101074, 0.4547915458679199],\n",
       "  [0.9382673501968384, 0.8263896107673645],\n",
       "  [0.7668588757514954, 0.11383813619613647]],\n",
       " [[0.20182007551193237, 0.11842131614685059],\n",
       "  [0.03093045949935913, 0.47815126180648804],\n",
       "  [0.1366625428199768, 0.25330251455307007],\n",
       "  [0.972752034664154, 0.43837404251098633],\n",
       "  [0.3885332942008972, 0.6771019697189331],\n",
       "  [0.24933010339736938, 0.023930490016937256],\n",
       "  [0.307674765586853, 0.07053065299987793],\n",
       "  [0.4967474937438965, 0.648440420627594],\n",
       "  [0.5424361824989319, 0.8717440962791443],\n",
       "  [0.3327009677886963, 0.7262998223304749],\n",
       "  [0.23592114448547363, 0.8830875158309937],\n",
       "  [0.3550448417663574, 0.8904604911804199]],\n",
       " [[0.7161503434181213, 0.322304904460907],\n",
       "  [0.9886513352394104, 0.8429857492446899],\n",
       "  [0.44463902711868286, 0.4317857623100281],\n",
       "  [0.2516695261001587, 0.612903356552124],\n",
       "  [0.6619264483451843, 0.39250481128692627],\n",
       "  [0.8753013014793396, 0.5064316391944885],\n",
       "  [0.6706973910331726, 0.6441148519515991],\n",
       "  [0.48589271306991577, 0.05274224281311035],\n",
       "  [0.519045352935791, 0.16296273469924927],\n",
       "  [0.29972022771835327, 0.9460693597793579],\n",
       "  [0.18826252222061157, 0.08723610639572144],\n",
       "  [0.7084996700286865, 0.9376965165138245]],\n",
       " [[0.21750497817993164, 0.05851304531097412],\n",
       "  [0.2332097887992859, 0.3707966208457947],\n",
       "  [0.9578258395195007, 0.4473820924758911],\n",
       "  [0.6618911027908325, 0.1892978549003601],\n",
       "  [0.7348793148994446, 0.5666335225105286],\n",
       "  [0.08572989702224731, 0.9904494285583496],\n",
       "  [0.7330604791641235, 0.7410865426063538],\n",
       "  [0.47571688890457153, 0.4010471701622009],\n",
       "  [0.4022166132926941, 0.8018207550048828],\n",
       "  [0.03288376331329346, 0.7225730419158936],\n",
       "  [0.2419205904006958, 0.5348281860351562],\n",
       "  [0.2828187346458435, 0.39447951316833496]],\n",
       " [[0.8797000050544739, 0.17388612031936646],\n",
       "  [0.5122090578079224, 0.2728937864303589],\n",
       "  [0.43036556243896484, 0.709610104560852],\n",
       "  [0.9430999755859375, 0.18487322330474854],\n",
       "  [0.06823974847793579, 0.6444866061210632],\n",
       "  [0.3843876123428345, 0.14283478260040283],\n",
       "  [0.7126622200012207, 0.21222549676895142],\n",
       "  [0.4083791971206665, 0.6782103180885315],\n",
       "  [0.8283113837242126, 0.48409026861190796],\n",
       "  [0.38017845153808594, 0.33806324005126953],\n",
       "  [0.597602128982544, 0.3684384822845459],\n",
       "  [0.5549426674842834, 0.9244885444641113]],\n",
       " [[0.7301843762397766, 0.3667639493942261],\n",
       "  [0.3246801495552063, 0.6167309880256653],\n",
       "  [0.7457430362701416, 0.07350671291351318],\n",
       "  [0.4574008584022522, 0.05175846815109253],\n",
       "  [0.969325065612793, 0.06402873992919922],\n",
       "  [0.4488140940666199, 0.7256600260734558],\n",
       "  [0.17875635623931885, 0.6729238033294678],\n",
       "  [0.9148029088973999, 0.21379029750823975],\n",
       "  [0.4398112893104553, 0.45118969678878784],\n",
       "  [0.14171558618545532, 0.5658754110336304],\n",
       "  [0.7254167795181274, 0.11544090509414673],\n",
       "  [0.4014333486557007, 0.1376047134399414]],\n",
       " [[0.8627140522003174, 0.43404895067214966],\n",
       "  [0.5953429341316223, 0.5906013250350952],\n",
       "  [0.3833118677139282, 0.06037360429763794],\n",
       "  [0.321505069732666, 0.5640701055526733],\n",
       "  [0.03828352689743042, 0.9771932363510132],\n",
       "  [0.07083934545516968, 0.32478946447372437],\n",
       "  [0.6461970806121826, 0.32387393712997437],\n",
       "  [0.4829638600349426, 0.8176276087760925],\n",
       "  [0.4967184066772461, 0.6689603924751282],\n",
       "  [0.41783493757247925, 0.30603569746017456],\n",
       "  [0.584978461265564, 0.241493821144104],\n",
       "  [0.6950649619102478, 0.8818369507789612]],\n",
       " [[0.5278763175010681, 0.2873169183731079],\n",
       "  [0.2818722128868103, 0.2520172595977783],\n",
       "  [0.4176732897758484, 0.0763312578201294],\n",
       "  [0.4948490858078003, 0.7980554699897766],\n",
       "  [0.41293203830718994, 0.6247667670249939],\n",
       "  [0.42429786920547485, 0.9954981803894043],\n",
       "  [0.06813597679138184, 0.6470561027526855],\n",
       "  [0.9265364408493042, 0.49042755365371704],\n",
       "  [0.864108145236969, 0.8722209334373474],\n",
       "  [0.32743990421295166, 0.7728254795074463],\n",
       "  [0.06080693006515503, 0.8797509074211121],\n",
       "  [0.0024796128273010254, 0.42337238788604736]],\n",
       " [[0.8347588777542114, 0.2618999481201172],\n",
       "  [0.40214812755584717, 0.2648565173149109],\n",
       "  [0.05426609516143799, 0.6673035621643066],\n",
       "  [0.2864255905151367, 0.39264339208602905],\n",
       "  [0.2987397313117981, 0.1962164044380188],\n",
       "  [0.42000842094421387, 0.5934136509895325],\n",
       "  [0.7115191221237183, 0.3533552885055542],\n",
       "  [0.3381151556968689, 0.956283688545227],\n",
       "  [0.05840480327606201, 0.6154913902282715],\n",
       "  [0.21958762407302856, 0.7068090438842773],\n",
       "  [0.1489393711090088, 0.9389822483062744],\n",
       "  [0.010618269443511963, 0.7891921997070312]],\n",
       " [[0.9031625986099243, 0.5746846199035645],\n",
       "  [0.7374333143234253, 0.7505376935005188],\n",
       "  [0.9929342865943909, 0.16042625904083252],\n",
       "  [0.8214770555496216, 0.9529294967651367],\n",
       "  [0.9856547713279724, 0.3633022904396057],\n",
       "  [0.32339024543762207, 0.30128657817840576],\n",
       "  [0.3039305806159973, 0.8830103278160095],\n",
       "  [0.37979429960250854, 0.9356650114059448],\n",
       "  [0.11335235834121704, 0.8428447246551514],\n",
       "  [0.6909417510032654, 0.023832857608795166],\n",
       "  [0.7272129058837891, 0.04239684343338013],\n",
       "  [0.8012614250183105, 0.3951230049133301]],\n",
       " [[0.5404825806617737, 0.9982209205627441],\n",
       "  [0.543380856513977, 0.8962242603302002],\n",
       "  [0.22476476430892944, 0.4636862277984619],\n",
       "  [0.42296016216278076, 0.8237032890319824],\n",
       "  [0.2508993148803711, 0.5736754536628723],\n",
       "  [0.8104787468910217, 0.4678729176521301],\n",
       "  [0.1858488917350769, 0.2281714677810669],\n",
       "  [0.08445072174072266, 0.44231247901916504],\n",
       "  [0.7230706810951233, 0.6884338855743408],\n",
       "  [0.528298556804657, 0.912007212638855],\n",
       "  [0.4214939475059509, 0.2523161768913269],\n",
       "  [0.25037986040115356, 0.9388822317123413]],\n",
       " [[0.5108742713928223, 0.41192495822906494],\n",
       "  [0.34163790941238403, 0.9518628120422363],\n",
       "  [0.021038413047790527, 0.012830913066864014],\n",
       "  [0.4677315354347229, 0.15331655740737915],\n",
       "  [0.3841785788536072, 0.3521420359611511],\n",
       "  [0.06431907415390015, 0.8575318455696106],\n",
       "  [0.7605745792388916, 0.9467661380767822],\n",
       "  [0.9285450577735901, 0.2766888737678528],\n",
       "  [0.747113823890686, 0.9495612978935242],\n",
       "  [0.14270347356796265, 0.5295022130012512],\n",
       "  [0.7025392651557922, 0.36735475063323975],\n",
       "  [0.4701181650161743, 0.17830222845077515]],\n",
       " [[0.3407300114631653, 0.6193392872810364],\n",
       "  [0.0475650429725647, 0.26717835664749146],\n",
       "  [0.5347641706466675, 0.8229882121086121],\n",
       "  [0.5454607605934143, 0.4499441385269165],\n",
       "  [0.47416114807128906, 0.1557680368423462],\n",
       "  [0.3045879602432251, 0.9732917547225952],\n",
       "  [0.5793803930282593, 0.41454988718032837],\n",
       "  [0.10170257091522217, 0.9749823808670044],\n",
       "  [0.5374884009361267, 0.4057123064994812],\n",
       "  [0.17650359869003296, 0.8375877737998962],\n",
       "  [0.8999061584472656, 0.7294188737869263],\n",
       "  [0.12503331899642944, 0.18966251611709595]],\n",
       " [[0.30768853425979614, 0.7384475469589233],\n",
       "  [0.015505492687225342, 0.6124745607376099],\n",
       "  [0.2754164934158325, 0.26438575983047485],\n",
       "  [0.10762578248977661, 0.11529338359832764],\n",
       "  [0.8399868607521057, 0.0983080267906189],\n",
       "  [0.17567497491836548, 0.6601385474205017],\n",
       "  [0.3813897967338562, 0.31370997428894043],\n",
       "  [0.8438876867294312, 0.760908305644989],\n",
       "  [0.09656882286071777, 0.6305789351463318],\n",
       "  [0.7717180848121643, 0.27529335021972656],\n",
       "  [0.07170194387435913, 0.1978778839111328],\n",
       "  [0.9695673584938049, 0.6807472109794617]],\n",
       " [[0.6820734739303589, 0.3296283483505249],\n",
       "  [0.929691731929779, 0.4719504117965698],\n",
       "  [0.4720638394355774, 0.8148122429847717],\n",
       "  [0.38946104049682617, 0.13920944929122925],\n",
       "  [0.3999266028404236, 0.9965012669563293],\n",
       "  [0.03151559829711914, 0.683695375919342],\n",
       "  [0.2629810571670532, 0.5920589566230774],\n",
       "  [0.2996246814727783, 0.7622617483139038],\n",
       "  [0.16636669635772705, 0.8613442182540894],\n",
       "  [0.37052619457244873, 0.9572741389274597],\n",
       "  [0.6941325068473816, 0.12122780084609985],\n",
       "  [0.24593818187713623, 0.8280490636825562]],\n",
       " [[0.9371556043624878, 0.7542567253112793],\n",
       "  [0.9308524131774902, 0.3568161129951477],\n",
       "  [0.6175740957260132, 0.1954329013824463],\n",
       "  [0.16908174753189087, 0.21490401029586792],\n",
       "  [0.23303478956222534, 0.3989086151123047],\n",
       "  [0.5768462419509888, 0.5319854617118835],\n",
       "  [0.7363356947898865, 0.45761239528656006],\n",
       "  [0.7082576751708984, 0.27395617961883545],\n",
       "  [0.37607908248901367, 0.9095130562782288],\n",
       "  [0.77394700050354, 0.5109514594078064],\n",
       "  [0.44006073474884033, 0.6285182237625122],\n",
       "  [0.2616862654685974, 0.0017880797386169434]],\n",
       " [[0.959611177444458, 0.5030536651611328],\n",
       "  [0.30129337310791016, 0.2564029097557068],\n",
       "  [0.008641660213470459, 0.4910498261451721],\n",
       "  [0.583222508430481, 0.5573387742042542],\n",
       "  [0.6468589305877686, 0.9469466209411621],\n",
       "  [0.2881196141242981, 0.6409218311309814],\n",
       "  [0.8361781239509583, 0.28177350759506226],\n",
       "  [0.9233126044273376, 0.2853601574897766],\n",
       "  [0.7532985806465149, 0.7375539541244507],\n",
       "  [0.4353417158126831, 0.42532598972320557],\n",
       "  [0.6114064455032349, 0.3754226565361023],\n",
       "  [0.4073583483695984, 0.1441829800605774]],\n",
       " [[0.7235710024833679, 0.5947197675704956],\n",
       "  [0.7116502523422241, 0.6465832591056824],\n",
       "  [0.603962242603302, 0.9792604446411133],\n",
       "  [0.8039786219596863, 0.6985043883323669],\n",
       "  [0.5886170864105225, 0.723453938961029],\n",
       "  [0.8108099102973938, 0.6042496562004089],\n",
       "  [0.34363681077957153, 0.07281911373138428],\n",
       "  [0.1700228452682495, 0.9344823956489563],\n",
       "  [0.08240348100662231, 0.31254297494888306],\n",
       "  [0.43593114614486694, 0.32732874155044556],\n",
       "  [0.40100598335266113, 0.5097236633300781],\n",
       "  [0.027260899543762207, 0.8049864768981934]],\n",
       " [[0.47277700901031494, 0.6791255474090576],\n",
       "  [0.10780620574951172, 0.1122627854347229],\n",
       "  [0.02433168888092041, 0.330624520778656],\n",
       "  [0.4789966344833374, 0.4015223979949951],\n",
       "  [0.17080432176589966, 0.7604144215583801],\n",
       "  [0.5211796164512634, 0.7280527353286743],\n",
       "  [0.6388103365898132, 0.5968531370162964],\n",
       "  [0.49665367603302, 0.1659032106399536],\n",
       "  [0.6172025203704834, 0.22284150123596191],\n",
       "  [0.3888063430786133, 0.7905269265174866],\n",
       "  [0.7062603235244751, 0.9073336720466614],\n",
       "  [0.40300238132476807, 0.7752321362495422]],\n",
       " [[0.6192333102226257, 0.6370730400085449],\n",
       "  [0.7485200762748718, 0.287470281124115],\n",
       "  [0.5166627168655396, 0.10006511211395264],\n",
       "  [0.188232421875, 0.03795671463012695],\n",
       "  [0.7020978927612305, 0.25785690546035767],\n",
       "  [0.31462836265563965, 0.4354298710823059],\n",
       "  [0.9249745011329651, 0.3554535508155823],\n",
       "  [0.8831756114959717, 0.5225762128829956],\n",
       "  [0.6011112332344055, 0.8235213160514832],\n",
       "  [0.1693097949028015, 0.6545971035957336],\n",
       "  [0.4148685336112976, 0.36934274435043335],\n",
       "  [0.21428126096725464, 0.8341085910797119]],\n",
       " [[0.869868814945221, 0.46720385551452637],\n",
       "  [0.22879564762115479, 0.2773452401161194],\n",
       "  [0.403831422328949, 0.37629562616348267],\n",
       "  [0.7579703330993652, 0.45576584339141846],\n",
       "  [0.2474212646484375, 0.81410151720047],\n",
       "  [0.11769688129425049, 0.2665184736251831],\n",
       "  [0.1928425431251526, 0.8797273635864258],\n",
       "  [0.2802128195762634, 0.9649653434753418],\n",
       "  [0.06826430559158325, 0.7477635741233826],\n",
       "  [0.19770735502243042, 0.14957034587860107],\n",
       "  [0.7632057666778564, 0.9613401889801025],\n",
       "  [0.5018313527107239, 0.8073033690452576]],\n",
       " [[0.7668329477310181, 0.3303513526916504],\n",
       "  [0.6718864440917969, 0.48857372999191284],\n",
       "  [0.32136690616607666, 0.4832664132118225],\n",
       "  [0.6269915699958801, 0.2339949607849121],\n",
       "  [0.14030921459197998, 0.7047443985939026],\n",
       "  [0.49281638860702515, 0.9941434860229492],\n",
       "  [0.9361689686775208, 0.16434091329574585],\n",
       "  [0.038116395473480225, 0.9638723134994507],\n",
       "  [0.5319609642028809, 0.4094235301017761],\n",
       "  [0.23000770807266235, 0.8444132804870605],\n",
       "  [0.7363225221633911, 0.32251662015914917],\n",
       "  [0.05674111843109131, 0.023492395877838135]],\n",
       " [[0.1657041311264038, 0.4702160954475403],\n",
       "  [0.1088445782661438, 0.534086287021637],\n",
       "  [0.9660806059837341, 0.881056010723114],\n",
       "  [0.07511401176452637, 0.756440281867981],\n",
       "  [0.23709636926651, 0.33542847633361816],\n",
       "  [0.3032262921333313, 0.1934816837310791],\n",
       "  [0.6850771903991699, 0.2765752077102661],\n",
       "  [0.659683883190155, 0.8021752238273621],\n",
       "  [0.6868954300880432, 0.5015074014663696],\n",
       "  [0.36836767196655273, 0.28495490550994873],\n",
       "  [0.34151583909988403, 0.43741369247436523],\n",
       "  [0.43776190280914307, 0.5386466979980469]],\n",
       " [[0.8168154358863831, 0.25619983673095703],\n",
       "  [0.8262196779251099, 0.4268531799316406],\n",
       "  [0.4830546975135803, 0.5843554139137268],\n",
       "  [0.7469015717506409, 0.8021708726882935],\n",
       "  [0.7383943796157837, 0.649308443069458],\n",
       "  [0.55063396692276, 0.3061959147453308],\n",
       "  [0.8984470367431641, 0.2697928547859192],\n",
       "  [0.44336795806884766, 0.17953342199325562],\n",
       "  [0.25586074590682983, 0.8391982913017273],\n",
       "  [0.3659871816635132, 0.0624578595161438],\n",
       "  [0.4683834910392761, 0.7666265368461609],\n",
       "  [0.2757663130760193, 0.18237006664276123]],\n",
       " [[0.295616090297699, 0.21266502141952515],\n",
       "  [0.2468886375427246, 0.8932621479034424],\n",
       "  [0.057698071002960205, 0.22548210620880127],\n",
       "  [0.8153427243232727, 0.1866949200630188],\n",
       "  [0.5360703468322754, 0.8070114254951477],\n",
       "  [0.22067278623580933, 0.8421688675880432],\n",
       "  [0.48162949085235596, 0.8387053608894348],\n",
       "  [0.8585330247879028, 0.46937745809555054],\n",
       "  [0.35885077714920044, 0.7202326655387878],\n",
       "  [0.9633497595787048, 0.7005031704902649],\n",
       "  [0.5411510467529297, 0.615943193435669],\n",
       "  [0.9749361872673035, 0.6105771064758301]],\n",
       " [[0.5817846655845642, 0.7176246643066406],\n",
       "  [0.2498319149017334, 0.7335708737373352],\n",
       "  [0.6860873699188232, 0.18947303295135498],\n",
       "  [0.3801996111869812, 0.9291592240333557],\n",
       "  [0.11349481344223022, 0.5523048639297485],\n",
       "  [0.8656185865402222, 0.6288904547691345],\n",
       "  [0.9227899312973022, 0.3255232572555542],\n",
       "  [0.6162500977516174, 0.34798794984817505],\n",
       "  [0.234305739402771, 0.30118054151535034],\n",
       "  [0.7103544473648071, 0.6725784540176392],\n",
       "  [0.5100056529045105, 0.5939355492591858],\n",
       "  [0.727694034576416, 0.14420545101165771]],\n",
       " [[0.4298258423805237, 0.14458680152893066],\n",
       "  [0.3130478262901306, 0.4884054660797119],\n",
       "  [0.6868376135826111, 0.07150375843048096],\n",
       "  [0.5399500131607056, 0.15686190128326416],\n",
       "  [0.7752780914306641, 0.9616037011146545],\n",
       "  [0.6524850130081177, 0.060240745544433594],\n",
       "  [0.15553408861160278, 0.019816935062408447],\n",
       "  [0.5381185412406921, 0.21743881702423096],\n",
       "  [0.17632347345352173, 0.05091047286987305],\n",
       "  [0.5075255036354065, 0.10915589332580566],\n",
       "  [0.894188404083252, 0.21473056077957153],\n",
       "  [0.55296391248703, 0.6916670203208923]],\n",
       " [[0.9850078225135803, 0.8478644490242004],\n",
       "  [0.49122142791748047, 0.6337378025054932],\n",
       "  [0.5349169373512268, 0.5604636073112488],\n",
       "  [0.870154857635498, 0.6610506772994995],\n",
       "  [0.7893663048744202, 0.42241495847702026],\n",
       "  [0.5484933257102966, 0.40162742137908936],\n",
       "  [0.17548227310180664, 0.32358598709106445],\n",
       "  [0.9223341941833496, 0.14570516347885132],\n",
       "  [0.6924039125442505, 0.8514766097068787],\n",
       "  [0.8487749099731445, 0.8606956005096436],\n",
       "  [0.3075786232948303, 0.3080832362174988],\n",
       "  [0.2649853229522705, 0.9559140801429749]],\n",
       " [[0.28475815057754517, 0.45674222707748413],\n",
       "  [0.6928941011428833, 0.7371456623077393],\n",
       "  [0.07066738605499268, 0.5016458034515381],\n",
       "  [0.3039567470550537, 0.4154239296913147],\n",
       "  [0.1580352783203125, 0.48441213369369507],\n",
       "  [0.05752748250961304, 0.3537134528160095],\n",
       "  [0.014119505882263184, 0.02464771270751953],\n",
       "  [0.6086936593055725, 0.4219224452972412],\n",
       "  [0.35039258003234863, 0.5590742230415344],\n",
       "  [0.07956439256668091, 0.6067834496498108],\n",
       "  [0.037780582904815674, 0.1396065354347229],\n",
       "  [0.6472206115722656, 0.32027941942214966]],\n",
       " [[0.3333330750465393, 0.9181452393531799],\n",
       "  [0.18240618705749512, 0.5707316398620605],\n",
       "  [0.018243849277496338, 0.9933673739433289],\n",
       "  [0.9261890053749084, 0.9850849509239197],\n",
       "  [0.1708781123161316, 0.21155661344528198],\n",
       "  [0.4600728750228882, 0.579104483127594],\n",
       "  [0.8496964573860168, 0.9527456760406494],\n",
       "  [0.5258225798606873, 0.7486148476600647],\n",
       "  [0.4407522678375244, 0.9077316522598267],\n",
       "  [0.30222153663635254, 0.12169498205184937],\n",
       "  [0.7950178980827332, 0.09572434425354004],\n",
       "  [0.2912048101425171, 0.40093863010406494]],\n",
       " [[0.014370083808898926, 0.8817256689071655],\n",
       "  [0.20622342824935913, 0.7986721396446228],\n",
       "  [0.692841649055481, 0.3638375997543335],\n",
       "  [0.5950441360473633, 0.3641364574432373],\n",
       "  [0.4681863784790039, 0.2995537519454956],\n",
       "  [0.2157776951789856, 0.609394907951355],\n",
       "  [0.339084267616272, 0.7116211652755737],\n",
       "  [0.28043603897094727, 0.4468623995780945],\n",
       "  [0.7884249687194824, 0.6327028274536133],\n",
       "  [0.556289553642273, 0.7047125697135925],\n",
       "  [0.3069230914115906, 0.9584479331970215],\n",
       "  [0.30665236711502075, 0.36205005645751953]],\n",
       " [[0.08796906471252441, 0.18813735246658325],\n",
       "  [0.890485942363739, 0.6036415100097656],\n",
       "  [0.7284020781517029, 0.19319301843643188],\n",
       "  [0.32465118169784546, 0.012488007545471191],\n",
       "  [0.811217188835144, 0.2631301283836365],\n",
       "  [0.8752948641777039, 0.660155177116394],\n",
       "  [0.48723161220550537, 0.35815542936325073],\n",
       "  [0.8434664607048035, 0.9925002455711365],\n",
       "  [0.010786592960357666, 0.7297167181968689],\n",
       "  [0.3753010034561157, 0.9023096561431885],\n",
       "  [0.03257298469543457, 0.2203301191329956],\n",
       "  [0.6183492541313171, 0.20541226863861084]],\n",
       " [[0.6873410940170288, 0.15189605951309204],\n",
       "  [0.33179450035095215, 0.2864665985107422],\n",
       "  [0.7932015657424927, 0.35195392370224],\n",
       "  [0.4172806143760681, 0.4589903950691223],\n",
       "  [0.16622334718704224, 0.09933573007583618],\n",
       "  [0.4550555944442749, 0.18379813432693481],\n",
       "  [0.6698452234268188, 0.5712031722068787],\n",
       "  [0.9025757312774658, 0.08364039659500122],\n",
       "  [0.23849964141845703, 0.6338733434677124],\n",
       "  [0.8745152354240417, 0.11651057004928589],\n",
       "  [0.6084762215614319, 0.5160244703292847],\n",
       "  [0.5891624093055725, 0.9245777726173401]],\n",
       " [[0.42366039752960205, 0.8325973749160767],\n",
       "  [0.18336039781570435, 0.3160848021507263],\n",
       "  [0.7272903323173523, 0.42520153522491455],\n",
       "  [0.6536012291908264, 0.2917960286140442],\n",
       "  [0.02253502607345581, 0.14347350597381592],\n",
       "  [0.04150032997131348, 0.7913700342178345],\n",
       "  [0.7774111032485962, 0.49791353940963745],\n",
       "  [0.5282720923423767, 0.21428543329238892],\n",
       "  [0.7982823252677917, 0.6784532070159912],\n",
       "  [0.9601502418518066, 0.15053218603134155],\n",
       "  [0.9002412557601929, 0.5931301712989807],\n",
       "  [0.969331681728363, 0.4737358093261719]]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e947113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tensy = torch.tensor(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ed975be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 12, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tensy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54c615",
   "metadata": {},
   "source": [
    "### span representation functions new with joined AM-AC spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "576cc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_representations(outputs, spans):\n",
    "    \n",
    "    \n",
    "    # 1. first things first: get the correct ADU spans using the trick you did earlier.\n",
    "    # spans here will be of the shape 48 x 24. we want 48 x 12 x 2\n",
    "    # the span shape of 48 x 24 checked. is correct. torch.size([48, 24])\n",
    "    # let's do that now:    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('spans shape avant reshape:', spans.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. ============ reshape the spans to 48 x 12 x 2\n",
    "    \n",
    "    batch_list = []\n",
    "\n",
    "    for sublist in spans:\n",
    "    \n",
    "        tmp_list = [list((sublist[i], sublist[i+1])) for i in range(0, len(sublist) - 1, 2)]\n",
    "        batch_list.append(tmp_list)\n",
    "    \n",
    "    spans = torch.tensor(batch_list)\n",
    "    \n",
    "    print('spans shape apres reshape:', spans.shape)\n",
    "    \n",
    "    # ================ reshape fin ==========\n",
    "    \n",
    "    batch_size = spans.shape[0]\n",
    "    nr_span_indices = spans.shape[1]\n",
    "    \n",
    "    # Step 2:\n",
    "    \n",
    "    adu_spans = spans[:,:,:] + 1 # Its the same. do we even need this? i think not.\n",
    "    \n",
    "    # Step 3:\n",
    "    \n",
    "    adu_spans = adu_spans.flatten(start_dim=1)\n",
    "    \n",
    "    print('adu spans shape:', adu_spans.shape)\n",
    "    \n",
    "    # Step 4:\n",
    "    \n",
    "    outputs_adus = outputs[:,adu_spans,:]\n",
    "    print('output_adus after slicing:', outputs_adus.shape)\n",
    "    outputs_adus = torch.cat([outputs_adus[i,i,:,:] for i in range(batch_size)], dim=0)\n",
    "    print('output_adus after cat:', outputs_adus.shape)\n",
    "    outputs_adus = outputs_adus.reshape(batch_size, nr_span_indices, -1)\n",
    "    \n",
    "    print(\"outputs adus shape:\", outputs_adus.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # print('nr span indices: ', nr_span_indices)\n",
    "    \n",
    "    idx_l_ams = range(0, nr_span_indices, 2) # [0,2,4,6 etc]\n",
    "    idx_l_acs = range(1, nr_span_indices, 2) # [1,3,5,7 etc]\n",
    "    \n",
    "    am_spans = spans[:, idx_l_ams, :] + 1 # adds 1 to all span indices (both am and ac) to offset for the CLS token in the input_ids.\n",
    "    ac_spans = spans[:, idx_l_acs, :] + 1\n",
    "    \n",
    "    am_spans = am_spans.flatten(start_dim=1)\n",
    "    ac_spans = ac_spans.flatten(start_dim=1)\n",
    "    \n",
    "    #print(\"am spans:\", am_spans.shape)\n",
    "    #print(\"ac spans:\", ac_spans.shape)\n",
    "    \n",
    "    outputs_am = outputs[:,am_spans,:]\n",
    "    outputs_am = torch.cat([outputs_am[i,i,:,:] for i in range(batch_size)], dim=0)\n",
    "    outputs_am = outputs_am.reshape(batch_size, -1, -1)\n",
    "    \n",
    "    # print(\"outputs am:\", outputs_am.shape)\n",
    "    \n",
    "    ### Now that we have outputs_am i.e. outputs at am_span indices, now create the four Kuri forumlas for AMs\n",
    "    \n",
    "    # ============== FIRST TERM ===================\n",
    "    \n",
    "    outputs_am_first_term = torch.cat([outputs_am[:,i+1,:] - outputs_am[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1) \n",
    "    outputs_am_first_term = outputs_am_first_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== SECOND TERM ==================\n",
    "    \n",
    "    outputs_am_second_term = torch.cat([outputs_am[:,i,:] - outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run # changed from +1 to +2 to ensure +2 is not a problem for AMs\n",
    "    outputs_am_second_term = outputs_am_second_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    \n",
    "    # ============== THIRD TERM ==================\n",
    "    \n",
    "    outputs_am_third_term = torch.cat([outputs_am[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "    outputs_am_third_term = outputs_am_third_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== FOURTH TERM ==================\n",
    "    \n",
    "    outputs_am_fourth_term = torch.cat([outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run # changed from +1 to +2 to ensure +2 is not a problem for AMs\n",
    "    outputs_am_fourth_term = outputs_am_fourth_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== NOW CONCATENATE THEM =========\n",
    "    \n",
    "    \n",
    "    am_minus_representations = torch.cat([outputs_am_first_term, outputs_am_second_term, outputs_am_third_term, outputs_am_fourth_term], dim=-1)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### am minus span representation according to kuribayashi paper is now here.\n",
    "    \n",
    "    ### ========================================= OLD AM CALCULATIONS ==========================\n",
    "    \n",
    "#     outputs_am_r = torch.cat([outputs_am[:,i,:] - outputs_am[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_am_r = outputs_am_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     outputs_am_l = torch.cat([outputs_am[:,i+1,:] - outputs_am[:,i,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_am_l = outputs_am_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     am_minus_representations = torch.cat([outputs_am_r, outputs_am_l], dim=-1)\n",
    "    \n",
    "    #print(\"am_minus_representations:\", am_minus_representations.shape)\n",
    "    \n",
    "    ### ====================================== FIN OLD AM CALCULATIONS =========================\n",
    "    \n",
    "    outputs_ac = outputs[:,ac_spans,:]\n",
    "    outputs_ac = torch.cat([outputs_ac[i,i,:,:] for i in range(batch_size)], dim=0)\n",
    "    outputs_ac = outputs_ac.reshape(batch_size, nr_span_indices, -1)\n",
    "    \n",
    "    #print(\"outputs ac:\", outputs_ac.shape)\n",
    "    \n",
    "    ### Now that we have outputs_ac i.e. outputs at ac_span indices, now create the four Kuri forumlas for ACs\n",
    "    \n",
    "    \n",
    "    # ============== FIRST TERM ===================\n",
    "    \n",
    "    outputs_ac_first_term = torch.cat([outputs_ac[:,i+1,:] - outputs_ac[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "    outputs_ac_first_term = outputs_ac_first_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== SECOND TERM ==================\n",
    "    \n",
    "    outputs_ac_second_term = torch.cat([outputs_ac[:,i,:] - outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run\n",
    "    outputs_ac_second_term = outputs_ac_second_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    \n",
    "    # ============== THIRD TERM ==================\n",
    "    \n",
    "    outputs_ac_third_term = torch.cat([outputs_ac[:,i-1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "    outputs_ac_third_term = outputs_ac_third_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== FOURTH TERM ==================\n",
    "    \n",
    "    outputs_ac_fourth_term = torch.cat([outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1) # changed + 2 to + 1 to make it run\n",
    "    outputs_ac_fourth_term = outputs_ac_fourth_term.reshape(batch_size, -1, 768)\n",
    "    \n",
    "    # ============== NOW CONCATENATE THEM =========\n",
    "    \n",
    "    \n",
    "    ac_minus_representations = torch.cat([outputs_ac_first_term, outputs_ac_second_term, outputs_ac_third_term, outputs_ac_fourth_term], dim=-1)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ac minus span representation according to kuribayashi paper is now here.\n",
    "    \n",
    "    ### ========================================= OLD AC CALCULATIONS ==========================\n",
    "    \n",
    "#     outputs_ac_r = torch.cat([outputs_ac[:,i,:] - outputs_ac[:,i+1,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_ac_r = outputs_ac_r.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     outputs_ac_l = torch.cat([outputs_ac[:,i+1,:] - outputs_ac[:,i,:] for i in range(0, nr_span_indices, 2)], dim=1)\n",
    "#     outputs_ac_l = outputs_ac_l.reshape(batch_size, -1, 768)\n",
    "    \n",
    "#     ac_minus_representations = torch.cat([outputs_ac_r, outputs_ac_l], dim=-1)\n",
    "\n",
    "    ### ====================================== FIN OLD AC CALCULATIONS =========================\n",
    "    \n",
    "    #print(\"ac_minus_representations:\", ac_minus_representations.shape)\n",
    "    \n",
    "    return am_minus_representations, ac_minus_representations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab13c4",
   "metadata": {},
   "source": [
    "## custom BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d24ebe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTKuri(nn.Module):\n",
    "\n",
    "    def __init__(self, first_model, model_am, model_ac, nr_classes):\n",
    "        \n",
    "        super(CustomBERTKuri, self).__init__()\n",
    "        \n",
    "        self.first_model = first_model\n",
    "        \n",
    "        self.intermediate_linear_am = nn.Linear(768 * 4, 768)\n",
    "        self.intermediate_linear_ac = nn.Linear(768 * 4, 768)        \n",
    "        \n",
    "        self.model_am = model_am\n",
    "        self.model_ac = model_ac\n",
    "        \n",
    "        self.nr_classes = nr_classes\n",
    "                \n",
    "        self.fc = nn.Linear(self.model_am.config.hidden_size + self.model_ac.config.hidden_size, self.nr_classes)        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        batch_tokenized, batch_spans = inputs \n",
    "        outputs = self.first_model(batch_tokenized)[0] # ** removed to correct error.\n",
    "        # spans = batch_spans # remove this spans thing cause we are now giving it just the spans themselves.\n",
    "        am_minus_representations, ac_minus_representations = get_span_representations(outputs, batch_spans)\n",
    "\n",
    "        am_minus_representations = self.intermediate_linear_am(am_minus_representations)\n",
    "        ac_minus_representations = self.intermediate_linear_ac(ac_minus_representations)\n",
    "\n",
    "        output_model_am = self.model_am(inputs_embeds = am_minus_representations)[0]\n",
    "        output_model_ac = self.model_ac(inputs_embeds = ac_minus_representations)[0]\n",
    "\n",
    "        adu_representations = torch.cat([output_model_am, output_model_ac], dim=-1)\n",
    "        # print(\"adu rep:\", adu_representations.shape)\n",
    "        output = self.fc(adu_representations)\n",
    "        # print(\"model class output avant reshape:\", output.shape)\n",
    "        # output = output.reshape(-1, self.nr_classes)\n",
    "        # print(\"model class output apres:\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a3c50",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d772cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 1\n",
    "BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e519db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_model = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78ef6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_am = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "109559bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ac = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcc03353",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomBERTKuri(first_model, model_am, model_ac, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9b5fe79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERTKuri(\n",
       "  (first_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (intermediate_linear_am): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (intermediate_linear_ac): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (model_am): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (model_ac): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1536, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ade8ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(ignore_index=- 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72e09a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(custom_model.parameters(), lr=9.999999999999997e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a7d9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8738174228603844e-05\n",
    "# best learning rate found by the whole leslie business\n",
    "# new best LR found= 9.999999999999997e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2cc54dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_BATCHES = len(dataset['train']) / BATCH_SIZE\n",
    "num_training_steps = NB_EPOCHS * NR_BATCHES\n",
    "num_warmup_steps = int(0.2 * num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6718a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    # return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e51bf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented for LR Finder. remove it from optimizer.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b54c8",
   "metadata": {},
   "source": [
    "### create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a089f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset['validation'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset['test'], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e39dbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(list_of_lists):\n",
    "    return [x for sublist in list_of_lists for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e15ec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dummy_labels(test_preds, test_labels):\n",
    "    \n",
    "    idxes = []\n",
    "    test_labels_l = []\n",
    "    for idx, val in enumerate(test_labels):\n",
    "        if val != -100:\n",
    "            idxes.append(idx)\n",
    "            test_labels_l.append(val)\n",
    "    \n",
    "    test_preds_l = []\n",
    "    for idx, val in enumerate(test_preds):\n",
    "        for good_idx in idxes:\n",
    "            if idx == good_idx:\n",
    "                test_preds_l.append(val)\n",
    "        \n",
    "    return test_preds_l, test_labels_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af283206",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d58dcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb8e0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ab227a6",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 27,
     "id": "521987db-56d5-4e71-95af-7d1fb4a06e3f",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "#     labels = inputs[\"labels\"]\n",
    "#     labels = labels.flatten() # xxx.\n",
    "\n",
    "\n",
    "\n",
    "#     outputs = model(inputs['input_ids'], inputs['spans'])\n",
    "#     outputs = outputs.flatten(0,1) # xxx. for the 4 x 12 , 3 output of the main class.\n",
    "\n",
    "\n",
    "#     loss_fct = nn.CrossEntropyLoss()#(weight=class_weights)\n",
    "#     # loss = loss_fct(outputs, labels.flatten())\n",
    "#     loss = loss_fct(outputs, labels) # xxx\n",
    "#     #print(\"loss:\", loss)\n",
    "#     return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fcf8d67",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 28,
     "id": "06697293-0ef4-4d24-95a9-6ca0ed569b51",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metric = load_metric('f1')\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "\n",
    "#     logits, labels = eval_pred\n",
    "\n",
    "    \n",
    "#     print(\"logits\", logits.shape)    \n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "#     print(\"preds:\", predictions.shape)\n",
    "    \n",
    "#     return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a069ac",
   "metadata": {},
   "source": [
    "### LR Finder Leslie Smith "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ddb1c16",
   "metadata": {},
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dataset):\n",
    "        'Initialization'\n",
    "        self.inputs = dataset['input_ids'], dataset['spans']\n",
    "        self.labels = dataset['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # ID = self.inputs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.inputs[0][index], self.inputs[1][index] \n",
    "        # X = torch.load('data/' + ID + '.pt')\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16ebc688",
   "metadata": {},
   "source": [
    "custom_train_dataset = CustomDataset(dataset['train'])\n",
    "custom_test_dataset = CustomDataset(dataset['test'])\n",
    "custom_val_dataset = CustomDataset(dataset['validation'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10f6ae72",
   "metadata": {},
   "source": [
    "train_dataloader = DataLoader(custom_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(custom_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(custom_val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95d54fac",
   "metadata": {},
   "source": [
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=- 100)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        input = input.flatten(0,1) \n",
    "        target = target.flatten()\n",
    "        return self.loss.forward(input, target)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "318fc0d4",
   "metadata": {},
   "source": [
    "loss = CustomCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a7192d2",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.AdamW(custom_model.parameters(), lr=1e-8, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a6b2fad",
   "metadata": {},
   "source": [
    "lr_finder = LRFinder(custom_model, optimizer, loss, device=device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88ee7b63",
   "metadata": {},
   "source": [
    "lr_finder.range_test(train_dataloader, val_loader=val_dataloader, step_mode='exp')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dc976d2",
   "metadata": {},
   "source": [
    "lr_finder.plot() # to inspect the loss-learning rate graph"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ab7de5d",
   "metadata": {},
   "source": [
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1418c0",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca046125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss=None, optimizer=None, train_dataloader=None, val_dataloader=None, nb_epochs=20):\n",
    "    \"\"\"Training loop\"\"\"\n",
    "\n",
    "    min_f1 = -torch.inf\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Iterrate over epochs\n",
    "    for e in range(nb_epochs):\n",
    "\n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_dataloader):            \n",
    "            \n",
    "            # unpack batch             \n",
    "            labels = batch['label'].to(device)\n",
    "            spans = batch['spans'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            \n",
    "            inputs = input_ids, spans\n",
    "            \n",
    "            # Reset gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute training loss\n",
    "            current_loss = loss(outputs.flatten(0,1), labels.flatten())\n",
    "            train_loss += current_loss.detach().item()\n",
    "\n",
    "            # Compute gradients\n",
    "            current_loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()            \n",
    "            \n",
    "            del batch\n",
    "        \n",
    "        scheduler.step()\n",
    "            \n",
    "        \n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Put model in eval mode\n",
    "        model.eval()\n",
    "        \n",
    "        preds_l = []\n",
    "        labels_l = []\n",
    "        \n",
    "        for batch in tqdm(val_dataloader):            \n",
    "            \n",
    "            # unpack batch             \n",
    "            labels = batch['label'].to(device)\n",
    "            spans = batch['spans'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            \n",
    "            inputs = input_ids, spans\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute validation loss\n",
    "            current_loss = loss(outputs.flatten(0,1), labels.flatten())\n",
    "            val_loss += current_loss.detach().item()\n",
    "            \n",
    "            preds_for_f1 = torch.argmax(outputs, dim=2).flatten().tolist()\n",
    "            labels_for_f1 = labels.flatten().tolist()\n",
    "            \n",
    "            preds_l.append(preds_for_f1)\n",
    "            labels_l.append(labels_for_f1)\n",
    "            \n",
    "            del batch\n",
    "        \n",
    "        # Prints\n",
    "        \n",
    "        preds_l = flatten_list(preds_l)\n",
    "        labels_l = flatten_list(labels_l)\n",
    "        \n",
    "        preds_l, labels_l = remove_dummy_labels(preds_l, labels_l)\n",
    "        \n",
    "        f1_score_epoch = f1_score(preds_l, labels_l, average='macro')        \n",
    "        \n",
    "        print(f\"Epoch {e+1}/{nb_epochs} \\\n",
    "                \\t Training Loss: {train_loss/len(train_dataloader):.3f} \\\n",
    "                \\t Validation Loss: {val_loss/len(val_dataloader):.3f} \\\n",
    "                \\t F1 score: {f1_score_epoch}\")\n",
    "        \n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        val_losses.append(val_loss/len(val_dataloader))\n",
    "        \n",
    "\n",
    "        # Save model if val loss decreases\n",
    "        if f1_score_epoch > min_f1:\n",
    "\n",
    "            min_f1 = f1_score_epoch\n",
    "            torch.save(model.first_model.state_dict(), 'first_model.pt')\n",
    "            torch.save(model.model_am.state_dict(), 'model_am.pt')\n",
    "            torch.save(model.model_ac.state_dict(), 'model_ac.pt')\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b4ed2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spans shape avant reshape: torch.Size([48, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spans shape apres reshape: torch.Size([48, 12, 2])\n",
      "adu spans shape: torch.Size([48, 24])\n",
      "output_adus after slicing: torch.Size([48, 48, 24, 768])\n",
      "output_adus after cat: torch.Size([1152, 768])\n",
      "outputs adus shape: torch.Size([48, 12, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only one dimension can be inferred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1393/4122650626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1393/1272961579.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss, optimizer, train_dataloader, val_dataloader, nb_epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Compute training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1393/1320415365.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# ** removed to correct error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# spans = batch_spans # remove this spans thing cause we are now giving it just the spans themselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mam_minus_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_minus_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_span_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mam_minus_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_linear_am\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mam_minus_representations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1393/3145702197.py\u001b[0m in \u001b[0;36mget_span_representations\u001b[0;34m(outputs, spans)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0moutputs_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mam_spans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0moutputs_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutputs_am\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0moutputs_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_am\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# print(\"outputs am:\", outputs_am.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only one dimension can be inferred"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(custom_model, loss, optimizer, train_dataloader, val_dataloader, NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "48 * 12 * 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ec1cb",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ced2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load best model\n",
    "# network_2 = Network()\n",
    "\n",
    "# network_2.load_state_dict(torch.load(path))\n",
    "# network_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c206a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_model = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))\n",
    "first_model.load_state_dict(torch.load('first_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f09db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_am = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))\n",
    "model_am.load_state_dict(torch.load('model_am.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ac = BertModel(BertConfig.from_pretrained(\"bert-base-uncased\"))\n",
    "model_ac.load_state_dict(torch.load('model_ac.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee868ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "\n",
    "custom_model_2 = CustomBERTKuri(first_model, model_am, model_ac, 3)\n",
    "custom_model_2.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "custom_model_2.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloader=None):\n",
    "    \n",
    "    \"\"\"Prediction loop\"\"\"\n",
    "\n",
    "    preds_l = []\n",
    "    labels_l = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch in test_dataloader:            \n",
    "            \n",
    "        # unpack batch             \n",
    "        labels = batch['label'].to(device).flatten().tolist()\n",
    "        spans = batch['spans'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        inputs = input_ids, spans\n",
    "\n",
    "        # get output\n",
    "        \n",
    "        raw_preds = model(inputs).to('cpu')\n",
    "        # print(raw_preds.shape)\n",
    "        raw_preds = raw_preds.detach()#.numpy()\n",
    "\n",
    "        # Compute argmax\n",
    "        \n",
    "        predictions = torch.argmax(raw_preds, dim=2).flatten().tolist()\n",
    "        preds_l.append(predictions)\n",
    "        labels_l.append(labels)        \n",
    "        \n",
    "        del batch\n",
    "            \n",
    "    return flatten_list(preds_l), flatten_list(labels_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b739030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_preds, test_labels = predict(custom_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(test_labels, test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove -100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afa37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_preds_l, test_labels_l = remove_dummy_labels(test_preds, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1486d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(test_labels_l, test_preds_l, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -100 done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95b2b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_preds, test_labels = predict(custom_model_2, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b300a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_l, test_labels_l = remove_dummy_labels(test_preds, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04268afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels_l, test_preds_l, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f92f671",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.897     0.677     0.772       155\n",
    "           1      0.577     0.640     0.607       303\n",
    "           2      0.869     0.875     0.872       805\n",
    "\n",
    "    accuracy                          0.794      1263\n",
    "   macro avg      0.781     0.731     0.750      1263\n",
    "weighted avg      0.803     0.794     0.796      1263\n",
    "\n",
    "40 epochs. learning rate: 1.8e-5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4941a625",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "           0      0.803     0.735     0.768       155\n",
    "           1      0.604     0.508     0.552       303\n",
    "           2      0.843     0.907     0.874       805\n",
    "\n",
    "    accuracy                          0.790      1263\n",
    "   macro avg      0.750     0.717     0.731      1263\n",
    "weighted avg      0.781     0.790     0.784      1263\n",
    "\n",
    "40 epochs. learning rate: 9.999997e-6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
