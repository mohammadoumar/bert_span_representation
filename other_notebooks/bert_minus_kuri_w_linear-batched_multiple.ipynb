{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4837c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.22.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: IProgress in /opt/conda/lib/python3.8/site-packages (0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from IProgress) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!pip install IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5d4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d299b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b891d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps:\n",
    "\n",
    "# 1. define a paragraph: two AMs, two ACs\n",
    "# 2. define spans of AMs and ACs\n",
    "# 3. tokenize\n",
    "# 7. put them in a BERT, get the output and do the pooling.\n",
    "# 8. \n",
    "# 8. concatenate the output\n",
    "# 9. give it to the combined bert\n",
    "# 10. get the FC layer result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a540e",
   "metadata": {},
   "source": [
    "### define batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c0de5",
   "metadata": {},
   "source": [
    "#### batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4df94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1 = \"\"\"\n",
    "Some people may argue that children will be more material,\n",
    "neglect their study for earning money or be exploited by the\n",
    "employers. However, if children get good care and\n",
    "instructions from their parents, they can take advantages of the\n",
    "work to learn valuable things and avoid going in a wrong way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e73dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_2 = \"\"\"\n",
    "Firstly, it is a fact that tourists from different cultures will probably cause changes to the \n",
    "cultural identity of the tourist destinations. For example, in the Vietnam War, many \n",
    "American soldiers came to Thailand for a break and involved in sexual and drug activities. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba4eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_texts = []\n",
    "batch_texts.append(paragraph_1)\n",
    "batch_texts.append(paragraph_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba55a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define spans\n",
    "# these spans are inclusive\n",
    "\n",
    "span_1 = []\n",
    "\n",
    "am_1_span = (0, 4)\n",
    "ac_1_span = (5, 23)\n",
    "\n",
    "am_2_span = (24, 25)\n",
    "ac_2_span = (26, 55)\n",
    "\n",
    "span_1.append(am_1_span)\n",
    "span_1.append(ac_1_span)\n",
    "span_1.append(am_2_span)\n",
    "span_1.append(ac_2_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f0beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_2 = []\n",
    "\n",
    "am_1_span = (0, 6)\n",
    "ac_1_span = (7, 23)\n",
    "\n",
    "am_2_span = (24, 26)\n",
    "ac_2_span = (27, 48)\n",
    "\n",
    "span_2.append(am_1_span)\n",
    "span_2.append(ac_1_span)\n",
    "span_2.append(am_2_span)\n",
    "span_2.append(ac_2_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09071957",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_spans = []\n",
    "batch_spans.append(span_1)\n",
    "batch_spans.append(span_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa085b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = {\n",
    "    \n",
    "    'batch_texts' : batch_texts,\n",
    "    'batch_spans' : batch_spans\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d1871a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_texts': ['\\nSome people may argue that children will be more material,\\nneglect their study for earning money or be exploited by the\\nemployers. However, if children get good care and\\ninstructions from their parents, they can take advantages of the\\nwork to learn valuable things and avoid going in a wrong way.\\n',\n",
       "  '\\nFirstly, it is a fact that tourists from different cultures will probably cause changes to the \\ncultural identity of the tourist destinations. For example, in the Vietnam War, many \\nAmerican soldiers came to Thailand for a break and involved in sexual and drug activities. \\n'],\n",
       " 'batch_spans': [[(0, 4), (5, 23), (24, 25), (26, 55)],\n",
       "  [(0, 6), (7, 23), (24, 26), (27, 48)]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbd81d",
   "metadata": {},
   "source": [
    "#### batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb7ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1 = \"\"\"\n",
    "Firstly, it is a fact that tourists from different cultures will probably cause changes to the \n",
    "cultural identity of the tourist destinations. For example, in the Vietnam War, many \n",
    "American soldiers came to Thailand for a break and involved in sexual and drug activities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ad2e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_2 = \"\"\"\n",
    "Some people may argue that children will be more material,\n",
    "neglect their study for earning money or be exploited by the\n",
    "employers. However, if children get good care and\n",
    "instructions from their parents, they can take advantages of the\n",
    "work to learn valuable things and avoid going in a wrong way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e357f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_texts = []\n",
    "batch_texts.append(paragraph_1)\n",
    "batch_texts.append(paragraph_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3af0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_1 = []\n",
    "\n",
    "am_1_span = (0, 6)\n",
    "ac_1_span = (7, 23)\n",
    "\n",
    "am_2_span = (24, 26)\n",
    "ac_2_span = (27, 48)\n",
    "\n",
    "span_1.append(am_1_span)\n",
    "span_1.append(ac_1_span)\n",
    "span_1.append(am_2_span)\n",
    "span_1.append(ac_2_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edc291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define spans\n",
    "# these spans are inclusive\n",
    "\n",
    "span_2 = []\n",
    "\n",
    "am_1_span = (0, 4)\n",
    "ac_1_span = (5, 23)\n",
    "\n",
    "am_2_span = (24, 25)\n",
    "ac_2_span = (26, 55)\n",
    "\n",
    "span_2.append(am_1_span)\n",
    "span_2.append(ac_1_span)\n",
    "span_2.append(am_2_span)\n",
    "span_2.append(ac_2_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "349d22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_spans = []\n",
    "batch_spans.append(span_1)\n",
    "batch_spans.append(span_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62a5ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2 = {\n",
    "    \n",
    "    'batch_texts' : batch_texts,\n",
    "    'batch_spans' : batch_spans\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aa48806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_texts': ['\\nFirstly, it is a fact that tourists from different cultures will probably cause changes to the \\ncultural identity of the tourist destinations. For example, in the Vietnam War, many \\nAmerican soldiers came to Thailand for a break and involved in sexual and drug activities.\\n',\n",
       "  '\\nSome people may argue that children will be more material,\\nneglect their study for earning money or be exploited by the\\nemployers. However, if children get good care and\\ninstructions from their parents, they can take advantages of the\\nwork to learn valuable things and avoid going in a wrong way.\\n'],\n",
       " 'batch_spans': [[(0, 6), (7, 23), (24, 26), (27, 48)],\n",
       "  [(0, 4), (5, 23), (24, 25), (26, 55)]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27f60453",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [batch_1, batch_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aad485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_texts': ['\\nSome people may argue that children will be more material,\\nneglect their study for earning money or be exploited by the\\nemployers. However, if children get good care and\\ninstructions from their parents, they can take advantages of the\\nwork to learn valuable things and avoid going in a wrong way.\\n',\n",
       "  '\\nFirstly, it is a fact that tourists from different cultures will probably cause changes to the \\ncultural identity of the tourist destinations. For example, in the Vietnam War, many \\nAmerican soldiers came to Thailand for a break and involved in sexual and drug activities. \\n'],\n",
       " 'batch_spans': [[(0, 4), (5, 23), (24, 25), (26, 55)],\n",
       "  [(0, 6), (7, 23), (24, 26), (27, 48)]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1377521",
   "metadata": {},
   "source": [
    "### tokenize batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7292df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch_texts):\n",
    "    \n",
    "    tokenized_batch = []\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        # print(text)\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "        tokenized_batch.append(tokenized_text)\n",
    "        \n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e82292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nSome people may argue that children will be more material,\\nneglect their study for earning money or be exploited by the\\nemployers. However, if children get good care and\\ninstructions from their parents, they can take advantages of the\\nwork to learn valuable things and avoid going in a wrong way.\\n', '\\nFirstly, it is a fact that tourists from different cultures will probably cause changes to the \\ncultural identity of the tourist destinations. For example, in the Vietnam War, many \\nAmerican soldiers came to Thailand for a break and involved in sexual and drug activities. \\n']\n",
      "['\\nFirstly, it is a fact that tourists from different cultures will probably cause changes to the \\ncultural identity of the tourist destinations. For example, in the Vietnam War, many \\nAmerican soldiers came to Thailand for a break and involved in sexual and drug activities.\\n', '\\nSome people may argue that children will be more material,\\nneglect their study for earning money or be exploited by the\\nemployers. However, if children get good care and\\ninstructions from their parents, they can take advantages of the\\nwork to learn valuable things and avoid going in a wrong way.\\n']\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    \n",
    "    tokenized_batches = []\n",
    "    \n",
    "    print(batch['batch_texts'])\n",
    "    \n",
    "    tokenized_batch = tokenize_batch(batch['batch_texts']) \n",
    "    \n",
    "    tokenized_batches.append(tokenized_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7fb6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'input_ids': tensor([[  101, 15847,  1010,  2009,  2003,  1037,  2755,  2008,  9045,  2013,\n",
       "            2367,  8578,  2097,  2763,  3426,  3431,  2000,  1996,  3451,  4767,\n",
       "            1997,  1996,  7538, 14345,  1012,  2005,  2742,  1010,  1999,  1996,\n",
       "            5148,  2162,  1010,  2116,  2137,  3548,  2234,  2000,  6504,  2005,\n",
       "            1037,  3338,  1998,  2920,  1999,  4424,  1998,  4319,  3450,  1012,\n",
       "             102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1]])},\n",
       "  {'input_ids': tensor([[  101,  2070,  2111,  2089,  7475,  2008,  2336,  2097,  2022,  2062,\n",
       "            3430,  1010, 19046,  2037,  2817,  2005,  7414,  2769,  2030,  2022,\n",
       "           18516,  2011,  1996, 12433,  1012,  2174,  1010,  2065,  2336,  2131,\n",
       "            2204,  2729,  1998,  8128,  2013,  2037,  3008,  1010,  2027,  2064,\n",
       "            2202, 12637,  1997,  1996,  2147,  2000,  4553,  7070,  2477,  1998,\n",
       "            4468,  2183,  1999,  1037,  3308,  2126,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53cca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok words and tokens and spans are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84ae9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_representation(output, span_start, span_end):\n",
    "    \n",
    "    # tokenized_input = tokenizer(PARAGRAPH, return_tensors=\"pt\")\n",
    "    # outputs = model(**tokenized_input, output_hidden_states=True)\n",
    "    \n",
    "    # hidden_states = outputs[1] #  outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    \n",
    "    # kuri-minus = h[j] - h[i-1]; h[i] - h[j+1]; h[i-1]; h[j+1]\n",
    "    \n",
    "    # first_term\n",
    "    \n",
    "    span_end_tensor = output[0][0][span_end] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_start_tensor = output[0][0][span_start - 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_first_term_tensor = span_end_tensor - span_start_tensor\n",
    "    \n",
    "    # second_term \n",
    "    \n",
    "    span_start_tensor_2 = output[0][0][span_start] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_end_tensor_2 = output[0][0][span_end + 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_second_term_tensor = span_start_tensor_2 - span_end_tensor_2\n",
    "    \n",
    "    # third_term \n",
    "    \n",
    "    span_tensor_3 = output[0][0][span_start - 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    \n",
    "    # fourth_term \n",
    "    \n",
    "    span_tensor_4 = output[0][0][span_end + 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    span_minus_tensor = torch.cat((span_first_term_tensor, span_second_term_tensor, span_tensor_3, span_tensor_4))\n",
    "    \n",
    "    return span_minus_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1b9060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTKuri(nn.Module):\n",
    "\n",
    "    def __init__(self, first_model, model_am, model_ac, nr_classes):\n",
    "        \n",
    "        super(CustomBERTKuri, self).__init__()\n",
    "        \n",
    "        self.first_model = first_model\n",
    "        \n",
    "        self.intermediate_linear_am = nn.Linear(3072, 768)\n",
    "        self.intermediate_linear_ac = nn.Linear(3072, 768)\n",
    "        \n",
    "        \n",
    "        self.model_am = model_am\n",
    "        self.model_ac = model_ac\n",
    "        \n",
    "        self.nr_classes = nr_classes\n",
    "                \n",
    "        self.fc = nn.Linear(self.model_am.config.hidden_size + self.model_ac.config.hidden_size, self.nr_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, tokenized_batch, batch_spans):\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for idx, tokenized_text in enumerate(tokenized_batch):\n",
    "        \n",
    "            text_output = self.first_model(**tokenized_text)\n",
    "            print(\"text output\", text_output[0][0].shape)\n",
    "\n",
    "            # print(paragraph_output[0].shape)\n",
    "\n",
    "            #output_am_1 = self.first_model(**am_1_tokenized_input)\n",
    "            #output_am_2 = self.first_model(**am_2_tokenized_input)\n",
    "\n",
    "            #output_ac_1 = self.first_model(**ac_1_tokenized_input)\n",
    "            #output_ac_2 = self.first_model(**ac_2_tokenized_input)\n",
    "            \n",
    "            \n",
    "\n",
    "            am_1_span_representation = get_span_representation(text_output, batch_spans[idx][0][0] + 1, batch_spans[idx][0][1] + 1)\n",
    "            am_2_span_representation = get_span_representation(text_output, batch_spans[idx][1][0] + 1, batch_spans[idx][1][1] + 1)\n",
    "\n",
    "            ac_1_span_representation = get_span_representation(text_output, batch_spans[idx][2][0] + 1, batch_spans[idx][2][1] + 1)\n",
    "            ac_2_span_representation = get_span_representation(text_output, batch_spans[idx][3][0] + 1, batch_spans[idx][3][1] + 1)\n",
    "\n",
    "            # print(am_1_span_representation.shape)\n",
    "\n",
    "\n",
    "            am_1_span_representation = self.intermediate_linear_am(am_1_span_representation)\n",
    "            am_2_span_representation = self.intermediate_linear_am(am_2_span_representation)\n",
    "\n",
    "            ac_1_span_representation = self.intermediate_linear_ac(ac_1_span_representation)\n",
    "            ac_2_span_representation = self.intermediate_linear_ac(ac_2_span_representation)\n",
    "\n",
    "            # print(am_1_span_representation.shape)\n",
    "            # print(ac_1_span_representation.shape)\n",
    "\n",
    "            # the linear business works in batching mode if done already.\n",
    "            # do it already               \n",
    "\n",
    "            am_tensor = torch.vstack([am_1_span_representation, am_2_span_representation])    \n",
    "            am_tensor = am_tensor.unsqueeze_(0)             \n",
    "            am_tensor = am_tensor.expand(1,2,768)\n",
    "\n",
    "            # print(am_tensor.shape)\n",
    "\n",
    "            ac_tensor = torch.vstack([ac_1_span_representation, ac_2_span_representation])    \n",
    "            ac_tensor = ac_tensor.unsqueeze_(0)            \n",
    "            ac_tensor = ac_tensor.expand(1,2,768)\n",
    "\n",
    "            # print(ac_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "            # print(batched_tensor.shape)\n",
    "\n",
    "            # output_am_1_span_rep = self.model_am(inputs_embeds = am_1_span_representation)\n",
    "\n",
    "            # batching is ok. 1 x 2 x 3072\n",
    "\n",
    "            # first question is: what about the CLS/SEP? so this question is solved\n",
    "            # second question is: 4 AMs. 1 x 2 x 3072\n",
    "\n",
    "\n",
    "            # am_tensor is of shape 2 x 768. to get the first one, we can do am_tensor[0]\n",
    "\n",
    "            output_am_span_rep = self.model_am(inputs_embeds = am_tensor) # 1 x 2 x 768\n",
    "\n",
    "            # print('passed am model')\n",
    "\n",
    "            # print(output_am_span_rep[0].shape) # 1 x 2 x 768\n",
    "\n",
    "            # output_am_2_span_rep = self.model_am(inputs_embeds = am_2_span_representation)\n",
    "\n",
    "            output_ac_span_rep = self.model_ac(inputs_embeds = ac_tensor)\n",
    "\n",
    "            # print('passed ac model')\n",
    "\n",
    "            # print(output_ac_span_rep[0].shape)\n",
    "\n",
    "            # output_ac_2_span_rep = self.model_ac(inputs_embeds = ac_2_span_representation)\n",
    "\n",
    "            # change the rest of the 3 according to the same trick above\n",
    "\n",
    "            # until here\n",
    "\n",
    "            # concatenation with sequentiality\n",
    "\n",
    "\n",
    "            adu_representations = torch.cat([output_am_span_rep[0], output_ac_span_rep[0]], dim=2)\n",
    "            # print(adu_representations.shape)\n",
    "            # adu_2_representation = torch.cat([output_am_span_rep[0][0] + output_ac_span_rep[0][0]], dim=0)\n",
    "\n",
    "    #         adu_1_rep = get_concatenated_adu_representations(output_am_1_span_rep, output_ac_1_span_rep)\n",
    "    #         adu_2_rep = get_concatenated_adu_representations(output_am_2_span_rep, output_ac_2_span_rep)\n",
    "\n",
    "            # output = model_combined(inputs_embeds = output[0])\n",
    "\n",
    "            output = self.fc(adu_representations)\n",
    "            #output_2 = self.fc(adu_2_rep)\n",
    "\n",
    "            # print(output.shape)\n",
    "            \n",
    "            outputs.append(output)\n",
    "\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a0ba8",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "563ca277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_model = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10c510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_am = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc6ad580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c741386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.22.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_am.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12958e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ac = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c45bbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_combined = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31122ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomBERTKuri(first_model, model_am, model_ac, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45182c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text output torch.Size([51, 768])\n",
      "text output torch.Size([58, 768])\n"
     ]
    }
   ],
   "source": [
    "our_output = custom_model(tokenized_batch, batch_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96e6285b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135/2989344038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "our_output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "208cfb65",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomBERTKuri' object has no attribute 'text_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135/387466547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomBERTKuri' object has no attribute 'text_output'"
     ]
    }
   ],
   "source": [
    "custom_model.text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beeeca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
