{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7376928c-3adc-4718-818c-71e15b7d9080",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "# Finetune BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "78f728f1-f29f-466b-9dcb-b49b3ff8106b",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "- Use the clean dataset (v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6911470f-7e34-4a77-add3-00c9018da2a2",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a9eb3028-23c0-4af9-a130-c4869378fccb",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f409dc3a-e8f4-41f9-ae0f-3a5f74a64cf1",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# pytorch version '1.9.0a0+df837d0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks:\n",
    "\n",
    "# 1. Finetune for linked/not_linked using text (Done)\n",
    "# 2. Finetune for linked/not_linked using strct fts as text (Done)\n",
    "# 3. Idea: try giving the component class in both 1 and 2 and see if it improves! It should cause Claims\n",
    "# are the most linked component type. (Done! Good results!) (Not nice.)\n",
    "# 3.1 Check where you got the MC/CL/PREM information. If you got it from a pervious classifier,\n",
    "# then it's ok. otherwise, it's cheating.\n",
    "# Idea: Task 6 is the equivalent of this task. Should be good results also!\n",
    "# 4. Finetune of joined label using text (Done) (Semi nice results)\n",
    "# 5. Finetune for joined label using strct fts as text (Error!)\n",
    "# 6. Finetune for MC/CL/PREM using strct w linked (Error!) (Not nice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "031a2430-7ba4-431f-8e42-e31baf9ce991",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas==1.3.4 in /opt/conda/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (3.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets==1.15.1 in /opt/conda/lib/python3.8/site-packages (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2.26.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.70.14)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.21.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.11.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (3.8.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2022.11.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.3.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (1.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==1.15.1) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.15.1) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install transformers==4.12.5\n",
    "!pip install datasets==1.15.1\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "db736e14-03fb-4438-938d-c705d6786666",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "37286c02-6bf4-41f5-ab70-51515a054e7b",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "0c699094-439f-4dda-85a9-815e7948540c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_FOLDER = '/notebooks/Data/bert_sequence_classification'\n",
    "DATA_FILE = '/notebooks/link_identification_task/data/pe_dataset_w_strct_comp_type.pt'\n",
    "RESULTS_FOLDER = '/notebooks/Results/bert_sequence_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "35aada91-232a-421e-a28f-94b359c6d65d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "dc52e71e-65fa-4946-acdf-e4fffe9d0f79",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d352c1cd-abff-4b1e-b5e2-611288e4fddb",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "013a6d64-65e7-4189-a468-40ecfd8a6736",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "626737f9-ff56-4992-b72b-60750375e455",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "4fbfcc7b-55fb-456e-ab02-2ae975803046",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'text', 'labels', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'structural_fts_as_text', 'joined_label', 'strct_fts_w_linked', 'strct_fts_w_comp_type'],\n",
       "        num_rows: 4712\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['essay_nr', 'starting_idx', 'component_label', 'ending_idx', 'text', 'labels', 'split', 'essay', 'argument_bound_1', 'argument_bound_2', 'argument_id', 'label', 'structural_fts_as_text', 'joined_label', 'strct_fts_w_linked', 'strct_fts_w_comp_type'],\n",
       "        num_rows: 1138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "57c21a2e-83cd-4f4b-a19a-0bae29eb4794",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic: Some young adults want independence from their parents quickly, Sentence: There will not be such worries when young adults live in their own home, because parents will take care for them, Para Number: 2, First in Para: No, Last in Para: No, Is in Introduction: No, Is in Conclusion: No, Component Label: Premise'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['strct_fts_w_comp_type'][230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "8e792eee-d502-4904-b73c-983390df5178",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "492e0415-2634-47bb-88bf-665c130d032c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "ba2c9132-d013-4b26-be96-146cf024a45e",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "label_names = set(dataset['train']['joined_label'])\n",
    "label_nb = len(label_names)\n",
    "labels = ClassLabel(num_classes=label_nb, names=label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 10,
     "id": "2f1ce51b-18bc-40db-983f-3434b8651e45",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=5, names={'MC-NL', 'CL-L', 'PREM-NL', 'CL-NL', 'PREM-L'}, names_file=None, id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 11,
     "id": "5438b78c-249c-48ac-98e8-a87a2cc0c86d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['strct_fts_w_comp_type'], truncation=True, padding=True, max_length=512)\n",
    "    tokens['labels'] = labels.str2int(batch['joined_label'])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 12,
     "id": "cb8d8c9a-7539-4d02-9dc5-98bade787549",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f8f3c48d0d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020936012268066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e55a5267ee489cba97330d7afc39d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017943859100341797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa00adbe4d4c2ab6738768905a59ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 13,
     "id": "84abf830-f842-4e91-8d5c-7fcd9ea2942e",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 14,
     "id": "c7d70e98-c2e0-4055-9d52-74c67c199c72",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['argument_bound_1', 'argument_bound_2', 'argument_id', 'attention_mask', 'component_label', 'ending_idx', 'essay', 'essay_nr', 'input_ids', 'joined_label', 'label', 'labels', 'split', 'starting_idx', 'strct_fts_w_comp_type', 'strct_fts_w_linked', 'structural_fts_as_text', 'text', 'token_type_ids'],\n",
       "        num_rows: 4712\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['argument_bound_1', 'argument_bound_2', 'argument_id', 'attention_mask', 'component_label', 'ending_idx', 'essay', 'essay_nr', 'input_ids', 'joined_label', 'label', 'labels', 'split', 'starting_idx', 'strct_fts_w_comp_type', 'strct_fts_w_linked', 'structural_fts_as_text', 'text', 'token_type_ids'],\n",
       "        num_rows: 1138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "97da78cf-3d8a-4699-a4b7-94c15c05f72d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['train']['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "26e5ff54-014b-4b15-9b50-8593c942c0d8",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 15,
     "id": "e825c71a-23f8-4794-b114-729819def9ab",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']#.shuffle(seed=42)\n",
    "test_dataset = dataset['test']#.shuffle(seed=42)\n",
    "\n",
    "train_val_datasets = dataset['train'].train_test_split(train_size=0.8)\n",
    "train_dataset = train_val_datasets['train']\n",
    "val_dataset = train_val_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 16,
     "id": "e13433b1-343d-417d-bfbd-8bacb4c57d23",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "dataset_d = {}\n",
    "dataset_d['train'] = train_dataset\n",
    "dataset_d['test'] = test_dataset\n",
    "dataset_d['val'] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 17,
     "id": "a298a871-1a0d-4ed8-9a1c-1ed3795f1d74",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] topic : attending boys and girls in a same school would bring many advantages, sentence : in my opinion, boys and girls should study together in the sense that they have a chance to know each other and learn how to behave with an opposite sex, para number : 1, first in para : yes, last in para : no, is in introduction : yes, is in conclusion : no, component label : majorclaim [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['test'][945]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "aa1384b3-bd47-41a3-86b1-9cf814d47cdf",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['train']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "84df366a-6334-4e3a-902b-deb1c99491fb",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['val']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 20,
     "id": "406ad783-a570-4c26-8aa6-243e866b6fbf",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TEST'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "set(dataset_d['test']['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "cfb84398-aaaf-4a6e-9d08-266634ee92bf",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=5, names={'MC-NL', 'CL-L', 'PREM-NL', 'CL-NL', 'PREM-L'}, names_file=None, id=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "64e822da-0a45-4c84-a48a-897ffe73096c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 21,
     "id": "8e8a6c6c-c4cb-4d56-b43a-d7c56f4c3a74",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "NUM_LABELS = labels.num_classes\n",
    "BATCH_SIZE = 48\n",
    "NB_EPOCHS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 26,
     "id": "f10b5a62-8adf-4152-b198-cf6b6905fd5c",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019846677780151367,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 440473133,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1322090cfb4c848acb73b5feb79d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "12de4f6b-f64a-4f05-b3b2-e1160d241220",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "61407639-662f-4a04-83a3-21af58facacc",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1567ea44-a89c-4f60-9a90-32a42a7b3d8b",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "bdc84591-81bc-4d23-89e9-60cc03802e3d",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 2093, 3: 313, 0: 459, 1: 634, 4: 270})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(dataset_d['train']['labels'].tolist())\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a4a5fd47-b00a-4048-aa35-75f72cebcac7",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5599, 3.3013, 1.0000, 6.6869, 7.7519], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = [max(counter.values()) / counter[k] for k in sorted(counter.keys())]\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 27,
     "id": "521987db-56d5-4e71-95af-7d1fb4a06e3f",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/main_classes/trainer.html\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss_fct = nn.CrossEntropyLoss()#(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 28,
     "id": "06697293-0ef4-4d24-95a9-6ca0ed569b51",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0354762077331543,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2069,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576c85ce053c47acbc47fb4251930318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 29,
     "id": "956f8a45-f8bd-42a5-b829-5e2b0e82cf42",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # output\n",
    "    output_dir=RESULTS_FOLDER,          \n",
    "    \n",
    "    # params\n",
    "    num_train_epochs=NB_EPOCHS,               # nb of epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # cf. paper Sun et al.\n",
    "    learning_rate=1e-5,#2e-5,                 # cf. paper Sun et al.\n",
    "#     warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,                         # cf. paper Sun et al.\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    \n",
    "    # eval\n",
    "    evaluation_strategy=\"steps\",              # cf. paper Sun et al.\n",
    "    eval_steps=20,                            # cf. paper Sun et al.\n",
    "    \n",
    "    # log\n",
    "    logging_dir=\"/notebooks/Results/bert_sequence_classification/tb_logs\",  \n",
    "    logging_strategy='steps',\n",
    "    logging_steps=20,\n",
    "    \n",
    "    # save\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    # save_steps=20, # default 500\n",
    "    load_best_model_at_end=True,              # cf. paper Sun et al.\n",
    "    # metric_for_best_model='eval_loss' \n",
    "    metric_for_best_model='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 30,
     "id": "219545f7-5aff-4d0c-87e2-4ca28a6e7acc",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer( # Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 31,
     "id": "b524a80a-ccf0-41f4-a015-e4ba2913fdc4",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running training *****\n",
      "  Num examples = 3769\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='474' max='474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [474/474 07:41, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.657200</td>\n",
       "      <td>1.360222</td>\n",
       "      <td>0.143576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.253500</td>\n",
       "      <td>1.031955</td>\n",
       "      <td>0.291508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.948900</td>\n",
       "      <td>0.808460</td>\n",
       "      <td>0.291455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>0.720561</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.586279</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.522360</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.462008</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.422156</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.406550</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.390899</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.377896</td>\n",
       "      <td>0.548045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.374729</td>\n",
       "      <td>0.660008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>0.372215</td>\n",
       "      <td>0.660008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.361924</td>\n",
       "      <td>0.662983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.356246</td>\n",
       "      <td>0.698533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.352292</td>\n",
       "      <td>0.698533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>0.355625</td>\n",
       "      <td>0.688160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.344906</td>\n",
       "      <td>0.706027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.343106</td>\n",
       "      <td>0.704275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.346681</td>\n",
       "      <td>0.701057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.342756</td>\n",
       "      <td>0.703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.322900</td>\n",
       "      <td>0.341920</td>\n",
       "      <td>0.703555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.343075</td>\n",
       "      <td>0.703555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 943\n",
      "  Batch size = 48\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f80c5225-1fb3-4fd2-bd12-6233af54dfe8",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /notebooks/Results/bert_sequence_classification/checkpoint-best-pe-w-probs_1\n",
      "Configuration saved in /notebooks/Results/bert_sequence_classification/checkpoint-best-pe-w-probs_1/config.json\n",
      "Model weights saved in /notebooks/Results/bert_sequence_classification/checkpoint-best-pe-w-probs_1/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Results/bert_sequence_classification/checkpoint-best-pe-w-probs_1/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Results/bert_sequence_classification/checkpoint-best-pe-w-probs_1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# save best model\n",
    "trainer.save_model(os.path.join(\"/notebooks/Results/bert_sequence_classification\", 'checkpoint-best-pe-w-probs_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f7ddb2ad-8b2e-481c-a571-1d0ae91aa3e0",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "abcd6ae9-c2c7-4040-b18f-5ef5ac5db204",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # load model\n",
    "#model_file = os.path.join(\"/notebooks/Results/bert_sequence_classification\", 'checkpoint-best-pe-w-probs_1')\n",
    "# model_file = os.path.join(RESULTS_FOLDER, 'checkpoint-1500')\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained(model_file, num_labels=NUM_LABELS)\n",
    "#model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "5dd6b4ba-e232-445b-be03-d097ef390fec",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "- Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c40cd5da-b664-445c-b109-7ea2dd19fb33",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1138\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143/143 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "test_raw_preds, test_labels, _ = test_trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_raw_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "99b041ac-6cd7-4999-8c50-355d1c5097f3",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "0e3937ed-0792-44bf-86f5-8327e8a67940",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       MC-NL      1.000     1.000     1.000       140\n",
      "        CL-L      0.788     0.976     0.872       168\n",
      "     PREM-NL      0.863     1.000     0.927       624\n",
      "       CL-NL      0.940     0.589     0.724       107\n",
      "      PREM-L      0.000     0.000     0.000        99\n",
      "\n",
      "    accuracy                          0.871      1138\n",
      "   macro avg      0.718     0.713     0.705      1138\n",
      "weighted avg      0.801     0.871     0.828      1138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_name = labels.int2str([0,1,2,3,4])\n",
    "print(classification_report(test_labels, test_preds, target_names=target_name, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, ..., 2, 2, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MC'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.int2str(0).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component_type(x):\n",
    "    # print(x)\n",
    "    result = x.split(\"-\")[0]\n",
    "    # print(result)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_type(x):\n",
    "    \n",
    "    return x.split(\"-\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'CL-L',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'MC-NL',\n",
       " 'CL-NL',\n",
       " 'CL-NL',\n",
       " 'CL-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'CL-L',\n",
       " 'MC-NL',\n",
       " 'MC-NL',\n",
       " 'CL-L',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " 'PREM-L',\n",
       " 'PREM-NL',\n",
       " 'PREM-NL',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.int2str(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_components = list(map(get_component_type, labels.int2str(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_components = list(map(get_component_type, labels.int2str(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'CL',\n",
       " 'MC',\n",
       " 'MC',\n",
       " 'CL',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " 'PREM',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CL      1.000     1.000     1.000       275\n",
      "          MC      1.000     1.000     1.000       140\n",
      "        PREM      1.000     1.000     1.000       723\n",
      "\n",
      "    accuracy                          1.000      1138\n",
      "   macro avg      1.000     1.000     1.000      1138\n",
      "weighted avg      1.000     1.000     1.000      1138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels_components, test_preds_components, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_links = list(map(get_link_type, labels.int2str(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_links = list(map(get_link_type, labels.int2str(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L      0.788     0.614     0.691       267\n",
      "          NL      0.889     0.949     0.918       871\n",
      "\n",
      "    accuracy                          0.871      1138\n",
      "   macro avg      0.839     0.782     0.804      1138\n",
      "weighted avg      0.866     0.871     0.865      1138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels_links, test_preds_links, digits=3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "          CL      0.458     0.295     0.358       275\n",
    "          MC      0.410     0.550     0.470       140\n",
    "        PREM      0.789     0.844     0.816       723\n",
    "\n",
    "    accuracy                          0.675      1138\n",
    "   macro avg      0.552     0.563     0.548      1138\n",
    "weighted avg      0.662     0.675     0.662      1138\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           L      0.350     0.213     0.265       267\n",
    "          NL      0.785     0.878     0.829       871\n",
    "\n",
    "    accuracy                          0.722      1138\n",
    "   macro avg      0.567     0.546     0.547      1138\n",
    "weighted avg      0.683     0.722     0.697      1138\n",
    "\n",
    "with 'text' only."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "  not_linked       0.77      0.94      0.85       871\n",
    "      linked       0.32      0.09      0.15       267\n",
    "\n",
    "    accuracy                           0.74      1138\n",
    "   macro avg       0.55      0.52      0.50      1138\n",
    "weighted avg       0.67      0.74      0.68      1138\n",
    "\n",
    "linked/not_linked with 'text'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "      linked      0.613     0.558     0.584       267\n",
    "  not_linked      0.868     0.892     0.880       871\n",
    "\n",
    "    accuracy                          0.814      1138\n",
    "   macro avg      0.741     0.725     0.732      1138\n",
    "weighted avg      0.808     0.814     0.811      1138\n",
    "\n",
    "linked/not_linked with 'strct_fts_as_text'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "precision    recall  f1-score   support\n",
    "\n",
    "      linked      0.802     0.607     0.691       267\n",
    "  not_linked      0.888     0.954     0.920       871\n",
    "\n",
    "    accuracy                          0.873      1138\n",
    "   macro avg      0.845     0.780     0.805      1138\n",
    "weighted avg      0.868     0.873     0.866      1138\n",
    "\n",
    "linked/not_linked with 'strct_fts_w_comp_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7b130fcd-8306-4329-a813-f75fa133222e",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "source": [
    "- Train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "acd1c3d0-d60d-47f3-91b7-fdef40341215",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: component_label, argument_bound_2, split, ending_idx, strct_fts_w_comp_type, starting_idx, argument_bound_1, structural_fts_as_text, text, essay, argument_id, essay_nr, strct_fts_w_linked, joined_label.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3769\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='472' max='472' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [472/472 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "train_raw_preds, train_labels, _ = train_trainer.predict(train_dataset)\n",
    "train_preds = np.argmax(train_raw_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "8dba6c09-5b69-4ddd-b66d-43db303b01a5",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6183"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7ebb543e-5e41-416c-ae0e-00a7a8495dd3",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 5, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_334/1046444055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2096\u001b[0m             )\n\u001b[1;32m   2097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2099\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 5, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "target_name = labels.int2str([0,1,2])\n",
    "print(classification_report(train_labels, train_preds, target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "34797114-8521-49e7-8bc2-412ec6dbd999",
     "kernelId": "5ae81875-6bf9-4a4d-b9d2-28830bffd5f4"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
