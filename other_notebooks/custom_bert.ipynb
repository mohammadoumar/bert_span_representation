{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a3eaae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 88.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 90.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.2-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0\n",
      "  Downloading widgetsnbextension-4.0.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 93.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0\n",
      "  Downloading jupyterlab_widgets-3.0.3-py3-none-any.whl (384 kB)\n",
      "\u001b[K     |████████████████████████████████| 384 kB 86.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.2 jupyterlab-widgets-3.0.3 widgetsnbextension-4.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting IProgress\n",
      "  Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from IProgress) (1.16.0)\n",
      "Installing collected packages: IProgress\n",
      "Successfully installed IProgress-0.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!pip install IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e59d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0176ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# dataset and dataloaders\n",
    "# get span representation function\n",
    "# custom model class\n",
    "# custom training loop\n",
    "# instantiate model\n",
    "# train for some epochs\n",
    "# put model in eval\n",
    "# get predictions\n",
    "# classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebddb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "\n",
    "Some people may argue that children will be more material,\n",
    "neglect their study for earning money or be exploited by the\n",
    "employers. However, if children get good care and\n",
    "instructions from their parents, they can take advantages of the\n",
    "work to learn valuable things and avoid going in a wrong way\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83fffc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define spans\n",
    "\n",
    "am_1_span = (0, 4)\n",
    "ac_1_span = (5, 22)\n",
    "\n",
    "am_2_span = (23, 24)\n",
    "ac_2_span = (25, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce19062d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021346569061279297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba989a2cadac453c93c6f94231b1aad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016282081604003906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 28,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea66d30b317a4472877e291d4907e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017047405242919922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3534e9d907134e138c30ac181255cd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "543e3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(paragraph, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "338c5ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "716ed60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2070,  2111,  2089,  7475,  2008,  2336,  2097,  2022,  2062,\n",
       "          3430,  1010, 19046,  2037,  2817,  2005,  7414,  2769,  2030,  2022,\n",
       "         18516,  2011,  1996, 12433,  1012,  2174,  1010,  2065,  2336,  2131,\n",
       "          2204,  2729,  1998,  8128,  2013,  2037,  3008,  1010,  2027,  2064,\n",
       "          2202, 12637,  1997,  1996,  2147,  2000,  4553,  7070,  2477,  1998,\n",
       "          4468,  2183,  1999,  1037,  3308,  2126,   102]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4f622a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, so without CLS and SEP, we have input_ids of len 55. exactly as the paragraph length. correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed9468b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we can separate the AMs and ACs before the tokenization. which is the better way? \n",
    "# for now, do it after the tokenization and then discuss with Mr Cabessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52184891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset the tokizer input_ids to ignore the CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67322507",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input['input_ids'] = tokenized_input['input_ids'].index_select(1, torch.tensor(list(range(1, tokenized_input['input_ids'].shape[1] - 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "447ed826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2070,  2111,  2089,  7475,  2008,  2336,  2097,  2022,  2062,  3430,\n",
       "          1010, 19046,  2037,  2817,  2005,  7414,  2769,  2030,  2022, 18516,\n",
       "          2011,  1996, 12433,  1012,  2174,  1010,  2065,  2336,  2131,  2204,\n",
       "          2729,  1998,  8128,  2013,  2037,  3008,  1010,  2027,  2064,  2202,\n",
       "         12637,  1997,  1996,  2147,  2000,  4553,  7070,  2477,  1998,  4468,\n",
       "          2183,  1999,  1037,  3308,  2126]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c33a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25f200ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps I should do a function will will do all this data processing, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c62272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now get the tokenized inputs of the various spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44df928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_1_tokenized_span = tokenized_input['input_ids'].index_select(1, torch.tensor(list(range(am_1_span[0], am_1_span[1] + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae62b171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2070, 2111, 2089, 7475, 2008]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_1_tokenized_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd0ca2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_1_tokenized_span = tokenized_input['input_ids'].index_select(1, torch.tensor(list(range(ac_1_span[0], ac_1_span[1] + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cf8647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_2_tokenized_span = tokenized_input['input_ids'].index_select(1, torch.tensor(list(range(am_2_span[0], am_2_span[1] + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c6ae7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_2_tokenized_span = tokenized_input['input_ids'].index_select(1, torch.tensor(list(range(ac_2_span[0], ac_2_span[1] + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1d8d9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1010,  2065,  2336,  2131,  2204,  2729,  1998,  8128,  2013,  2037,\n",
       "          3008,  1010,  2027,  2064,  2202, 12637,  1997,  1996,  2147,  2000,\n",
       "          4553,  7070,  2477,  1998,  4468,  2183,  1999,  1037,  3308,  2126]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_2_tokenized_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now these should be given to the custom BERT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a5603df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_spans = [am_1_tokenized_span, am_2_tokenized_span, ac_1_tokenized_span, ac_2_tokenized_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49287f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just an example object. when properly done, this should include ALL tokenized spans in of all AMs/ACs in the paragraph.\n",
    "# this object should be returned by a big tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4d389a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2070, 2111, 2089, 7475, 2008]]),\n",
       " tensor([[1012, 2174]]),\n",
       " tensor([[ 2336,  2097,  2022,  2062,  3430,  1010, 19046,  2037,  2817,  2005,\n",
       "           7414,  2769,  2030,  2022, 18516,  2011,  1996, 12433]]),\n",
       " tensor([[ 1010,  2065,  2336,  2131,  2204,  2729,  1998,  8128,  2013,  2037,\n",
       "           3008,  1010,  2027,  2064,  2202, 12637,  1997,  1996,  2147,  2000,\n",
       "           4553,  7070,  2477,  1998,  4468,  2183,  1999,  1037,  3308,  2126]])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customBERTKuri(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_am, model_ac, model_combined, tokenized_spans, nb_classes):\n",
    "\n",
    "        super(customBERTKuri, self).__init__()\n",
    "        \n",
    "\n",
    "        self.model_am = model_am\n",
    "        self.model_ac = model_ac\n",
    "        self.model_combined = model_combined\n",
    "        \n",
    "        # don't need embedding anymore because we will use BERT embeddings.\n",
    "        \n",
    "        # self.embedding = embedding\n",
    "        \n",
    "        # self.embedding_dim = embedding.word_embeddings.embedding_dim\n",
    "        \n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        self.linear = torch.nn.Linear(self.embedding_dim, self.nb_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        # 1. Embedding \n",
    "  \n",
    "        # para_embedding = embedding(paragraph)\n",
    "    \n",
    "        # 2. AM bert model\n",
    "        \n",
    "        am_model_output = self.model_am(tokenized_spans['markers'])\n",
    "        \n",
    "        # let's say tokenized_spans would be an object where we can access AM spans with 'markers' key\n",
    "        # similarly for AC spans with 'components' key\n",
    "        \n",
    "        ac_model_output = self.model_ac(tokenized_spans['components'])\n",
    "        \n",
    "            \n",
    "        # 3. Get BERT minus span representations / pooling\n",
    "        \n",
    "        span_representation_am = get_span_representation(am_model_output)\n",
    "        span_representation_ac = get_span_representation(ac_model_output)\n",
    "        \n",
    "        # span_representation_am consists of vectors of shape nr_ams x 768 (or whatever bert hidden dim we set). in this example: 2 x 768\n",
    "        # similarly for span_representation_ac\n",
    "        \n",
    "        \n",
    "        # 4. concatenate them for the combined model\n",
    "        # one by one\n",
    "        \n",
    "        \n",
    "        bert_minus_adu_representations = get_concatenated_adu_representations(span_representation_am, span_representation_ac)\n",
    "        \n",
    "        # bert_minus_adu_representations will consists ordered concatenations of AMs and ACs.\n",
    "        # will be of 2 * shape nr_adus in para x 768 (or whatever bert hidden dim we set). in this example: 4 x 768\n",
    "        \n",
    "        \n",
    "        # 5. contextualize them in the combined model\n",
    "        \n",
    "        combined_model_output = self.model_combined(bert_minus_adu_representations)        \n",
    "                    \n",
    "        # 6. FC layer\n",
    "        \n",
    "        outputs = self.linear(combined_model_output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dataloader, epochs=1, iter_steps=100):\n",
    "\n",
    "\n",
    "    loss_l = []\n",
    "    n_iter = 0\n",
    "\n",
    "    #  loop over epochs\n",
    "    for epoch in range(int(epochs)):\n",
    "\n",
    "        # loop over batches\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            outputs = model(batch[\"text\"])               # outputs\n",
    "            labels = batch[\"label\"].type(torch.int64)    # labels (converted to int for the loss)\n",
    "\n",
    "            loss = criterion(outputs, labels)         # compute loss\n",
    "            optimizer.zero_grad()                     # reset optimizer gradient\n",
    "            loss.backward()                           # backward pass\n",
    "            optimizer.step()                          # gradient update\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "            if n_iter % iter_steps == 0:\n",
    "                print(\"Iteration: {iteration} Loss: {loss}\".format(iteration=n_iter, loss=loss.item()))\n",
    "                loss_l.append(loss.item())\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    return loss_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_representation(PARAGRAPH, SPAN_START, SPAN_END):\n",
    "    \n",
    "    tokenized_input = tokenizer(PARAGRAPH, return_tensors=\"pt\")\n",
    "    outputs = model(**tokenized_input, output_hidden_states=True)\n",
    "    \n",
    "    hidden_states = outputs[1] #  outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    \n",
    "    # kuri-minus = h[j] - h[i-1]; h[i] - h[j+1]; h[i-1]; h[j+1]\n",
    "    \n",
    "    # first_term\n",
    "    \n",
    "    span_end_tensor = outputs[1][0][0][SPAN_END] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_start_tensor = outputs[1][0][0][SPAN_START - 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_first_term_tensor = span_end_tensor - span_start_tensor\n",
    "    \n",
    "    # second_term \n",
    "    \n",
    "    span_start_tensor_2 = outputs[1][0][0][SPAN_START] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_end_tensor_2 = outputs[1][0][0][SPAN_END + 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    span_second_term_tensor = span_start_tensor_2 - span_end_tensor_2\n",
    "    \n",
    "    # third_term \n",
    "    \n",
    "    span_tensor_3 = outputs[1][0][0][SPAN_START - 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    \n",
    "    # fourth_term \n",
    "    \n",
    "    span_tensor_4 = outputs[1][0][0][SPAN_END + 1] # outputs[1] instead of [2], because we have no loss now, because we passed no labels\n",
    "    \n",
    "    span_minus_tensor = torch.cat((span_first_term_tensor, span_second_term_tensor, span_tensor_3, span_tensor_4))\n",
    "    \n",
    "    return span_minus_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e131a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concatenated_adu_representations(span_representation_am, span_representation_ac):\n",
    "    \n",
    "    adu_representations = []\n",
    "    \n",
    "    for i in range(len(span_representation_am)):\n",
    "        \n",
    "        adu_representation = torch.concat(span_representation_am[i], span_representation_ac[i])\n",
    "        adu_representations.append(adu_representation)\n",
    "        \n",
    "    return adu_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de2ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
